this is not good as omni is managins all cluster it seems omni api is ok, but the cluster own api dones exist as was not bootrap using talos but using omni so all of htem have the same api thats the omni one and omni dont accept the token, since it waws not bootrapp by talos no api exists, just the omni exist i see now why, i need a way to connecte cluster or else no svcs in other cluster can be reoslveed or a will$ kubectl apply -f  broke_job.yaml
job.batch/deploy-submariner-broker created have to use an ip for each cluster each ius not ideial 


$ kubectl -n submariner-k8s-broker get pods
NAME                             READY   STATUS   RESTARTS   AGE
deploy-submariner-broker-lhxxq   0/1     Error    0          7s

PAA3LIS@LIS-C-002L2 MINGW64 ~
$ kubectl -n submariner-k8s-broker describe pods
Name:             deploy-submariner-broker-lhxxq
Namespace:        submariner-k8s-broker
Priority:         0
Service Account:  submariner-broker-installer
Node:             klusterx-worker-infra-04/10.19.143.70
Start Time:       Fri, 30 Jan 2026 18:49:24 +0000
Labels:           batch.kubernetes.io/controller-uid=c29f8e9a-f26e-418b-a596-36526fa1f405
                  batch.kubernetes.io/job-name=deploy-submariner-broker
                  controller-uid=c29f8e9a-f26e-418b-a596-36526fa1f405
                  job-name=deploy-submariner-broker
Annotations:      <none>
Status:           Failed
IP:               10.244.5.242
IPs:
  IP:           10.244.5.242
Controlled By:  Job/deploy-submariner-broker
Containers:
  subctl:
    Container ID:  containerd://239b132323bbf4f66ff846a37d28579b159c8f0a24559221c7bab258e931cabd
    Image:         quay.io/submariner/subctl:0.22.0
    Image ID:      quay.io/submariner/subctl@sha256:3f78777149921cacee8a2200f8e39113f176c1637160b4ba02bbe704e9c4372f
    Port:          <none>
    Host Port:     <none>
    Command:
      subctl
    Args:
      deploy-broker
      --namespace
      submariner-k8s-broker
      --broker-url
      https://kubernetes.default.svc:443
      --globalnet
    State:          Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Fri, 30 Jan 2026 18:49:25 +0000
      Finished:     Fri, 30 Jan 2026 18:49:25 +0000
    Ready:          False
    Restart Count:  0
    Environment:
      KUBECONFIG:
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8fzrz (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False
  Initialized                 True
  Ready                       False
  ContainersReady             False
  PodScheduled                True
Volumes:
  kube-api-access-8fzrz:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    Optional:                false
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  16s   default-scheduler  Successfully assigned submariner-k8s-broker/deploy-submariner-broker-lhxxq to klusterx-worker-infra-04
  Normal  Pulled     16s   kubelet            Container image "quay.io/submariner/subctl:0.22.0" already present on machine
  Normal  Created    16s   kubelet            Created container: subctl
  Normal  Started    15s   kubelet            Started container subctl


Name:             deploy-submariner-broker-rgcql
Namespace:        submariner-k8s-broker
Priority:         0
Service Account:  submariner-broker-installer
Node:             klusterx-worker-infra-04/10.19.143.70
Start Time:       Fri, 30 Jan 2026 18:49:35 +0000
Labels:           batch.kubernetes.io/controller-uid=c29f8e9a-f26e-418b-a596-36526fa1f405
                  batch.kubernetes.io/job-name=deploy-submariner-broker
                  controller-uid=c29f8e9a-f26e-418b-a596-36526fa1f405
                  job-name=deploy-submariner-broker
Annotations:      <none>
Status:           Failed
IP:               10.244.5.109
IPs:
  IP:           10.244.5.109
Controlled By:  Job/deploy-submariner-broker
Containers:
  subctl:
    Container ID:  containerd://ba14ac6857115f03348a6be4c493433e2db58604d8e6d95aa5697a5f065ec6e6
    Image:         quay.io/submariner/subctl:0.22.0
    Image ID:      quay.io/submariner/subctl@sha256:3f78777149921cacee8a2200f8e39113f176c1637160b4ba02bbe704e9c4372f
    Port:          <none>
    Host Port:     <none>
    Command:
      subctl
    Args:
      deploy-broker
      --namespace
      submariner-k8s-broker
      --broker-url
      https://kubernetes.default.svc:443
      --globalnet
    State:          Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Fri, 30 Jan 2026 18:49:35 +0000
      Finished:     Fri, 30 Jan 2026 18:49:35 +0000
    Ready:          False
    Restart Count:  0
    Environment:
      KUBECONFIG:
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lxfvg (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False
  Initialized                 True
  Ready                       False
  ContainersReady             False
  PodScheduled                True
Volumes:
  kube-api-access-lxfvg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    Optional:                false
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  6s    default-scheduler  Successfully assigned submariner-k8s-broker/deploy-submariner-broker-rgcql to klusterx-worker-infra-04
  Normal  Pulled     6s    kubelet            Container image "quay.io/submariner/subctl:0.22.0" already present on machine
  Normal  Created    6s    kubelet            Created container: subctl
  Normal  Started    6s    kubelet            Started container subctl

PAA3LIS@LIS-C-002L2 MINGW64 ~
$ kubectl -n submariner-k8s-broker logs deploy-submariner-broker-rgcql
 ✗ Error building the cluster.Info for the default configuration: could not obtain the cluster name from kube config: {
  "clusters": {},
  "users": {},
  "contexts": {},
  "current-context": ""
}

subctl version: release-0.22-6820f0d17e2b


PAA3LIS@LIS-C-002L2 MINGW64 ~
$ cat  broke_job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: deploy-submariner-broker
  namespace: submariner-k8s-broker
spec:
  backoffLimit: 1
  template:
    spec:
      restartPolicy: Never
      serviceAccountName: submariner-broker-installer

      # Talos: must run as root
      securityContext:
        runAsUser: 0
        runAsGroup: 0

      containers:
      - name: subctl
        image: quay.io/submariner/subctl:0.22.0
        command: ["subctl"]
        args:
          - "deploy-broker"
          - "--namespace"
          - "submariner-k8s-broker"
          - "--broker-url"
          - "https://kubernetes.default.svc:443"
          - "--globalnet"
        env:
        # Forces in-cluster config, avoids Omni kubeconfig
        - name: KUBECONFIG
          value: ""
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]











I hear you. If Omni is the ONLY control-plane endpoint for every cluster, and cluster-local apiservers are not reachable directly, then:

A pod in a spoke cannot reach the broker’s apiserver at kubernetes.default.svc in the broker cluster (that DNS resolves to the spoke’s apiserver).
And Omni’s API won’t accept cluster ServiceAccount (SA) tokens from either side.

Under those constraints, a standard subctl join must talk to the broker API, but there is no network + auth path from the spoke to the broker unless you either:

use Omni (as the broker URL and with an Omni-issued token), or
provide a temporary out-of-band TCP path to the broker apiserver (tunnel/VIP) that the spokes can reach and that matches the broker’s apiserver cert.

There isn’t a third way—subctl join needs to reach the broker’s API to write the broker-side cluster records.
Below are two pragmatic, working paths. Pick whichever is acceptable in your environment.

✅ Path A — Bootstrap via Omni (minimal, temporary Omni usage)
Idea: Use Omni only during bootstrap to let spokes reach the broker’s API. You regenerate broker-info.subm with:

brokerURL: Omni API URL (the only reachable endpoint)
clientToken: an Omni-issued bearer token (NOT a cluster SA token) that has read/write only in the broker namespace for Submariner broker objects (least privilege)

After all spokes join and Submariner connectivity is up, you can later rotate the broker URL to an internal endpoint reachable over Submariner (optional).
Steps


Create a dedicated Omni credential with minimum RBAC against the broker cluster namespace submariner-k8s-broker:

Permissions needed: CRUD on Secrets/ConfigMaps in that namespace, reads on Submariner CRDs under submariner.io in the broker namespace; often simpler to allow the broker namespace broadly during bootstrap.
This is an Omni-side action (create a user/token or service account in Omni with those permissions).



Regenerate broker-info.subm on the broker cluster with:

brokerURL: the Omni API URL (e.g., https://omni.example.com:PORT)
clientToken.data["token"]: the Omni bearer token, base64-encoded
clientToken.data["ca.crt"]: the Omni API CA, base64-encoded (NOT the in-cluster CA)
clientToken.data["namespace"]: base64(submariner-k8s-broker)



Here’s a broker-side Job that writes and upserts the broker-info Secret to use Omni (you must paste the Omni CA and Omni token into a Secret first, or inject via env; example below uses a Secret so no secrets in clear YAML):
1 error occurred: * overriding "cluster.controlPlane.endpoint" is not allowed in the config patch
