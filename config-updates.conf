To implement a Well-Architected 4-LB production setup for 2026, we will use Cilium Gateway API with Protocol Isolation. This allows Consul to use its internal TLS (via Passthrough) while Rancher, Grafana, and Prometheus use standard HTTP/HTTPS on the same shared IP.
1. The Multi-Purpose Gateway (gateway.yaml)
This file defines the Shared Hub (IP .240). It is architected to handle both encrypted traffic for Consul and plain traffic for other tools.


yaml
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: shared-hub-gateway
  namespace: infra-system
  annotations:
    io.cilium/lb-ipam-ips: "192.168.1.240"
spec:
  gatewayClassName: cilium
  listeners:
  - name: http
    protocol: HTTP
    port: 80
    allowedRoutes: { namespaces: { from: All } }
  - name: https-passthrough
    protocol: TLS
    port: 443
    tls:
      mode: Passthrough # Allows Consul to use its own TLS without sharing certs
    allowedRoutes: { namespaces: { from: All } }




2. The Application Routes (Cross-Namespace)
These files sit in the tool-specific namespaces and "attach" themselves to the Hub.
Consul (TLS Passthrough):
yaml
apiVersion: gateway.networking.k8s.io/v1alpha2
kind: TLSRoute
metadata:
  name: consul-ui-route
  namespace: consul
spec:
  parentRefs: [{ name: shared-hub-gateway, namespace: infra-system }]
  hostnames: ["consul.corp.local"]
  rules:
  - backendRefs: [{ name: consul-ui, port: 443 }]
Use o código com cuidado.
Rancher, Grafana, Prometheus (HTTP/HTTPS):
yaml
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: infra-tools-route
  namespace: infra-system
spec:
  parentRefs: [{ name: shared-hub-gateway, namespace: infra-system }]
  hostnames: ["rancher.corp.local", "grafana.corp.local", "prometheus.corp.local"]
  rules:
  - matches: [{ path: { type: PathPrefix, value: / } }]
    backendRefs:
    - name: rancher-service # Update these to your actual service names
      port: 80


3. Consolidated Consul Helm (values.yaml)
This configures the remaining 3 LBs: Control Plane (.244), Mesh (.242), and Peering (.243).
yaml
global:
  name: consul
  datacenter: central
  acls:
    manageSystemACLs: true
  tls:
    enabled: true
    enableAutoEncrypt: true

# IP .100: Control Plane (xDS / gRPC)
server:
  replicas: 3
  exposeGossipAndRPC:
    enabled: true
    type: LoadBalancer
    annotations: 
      io.cilium/lb-ipam-ips: "192.168.1.100"
    # REQUIRED: Label for the Cilium IP Pool selector
    labels:
      io.cilium/lb-ipam-ips: "true"

# IP .82: Mesh Gateway (Cross-cluster Service Data)
meshGateway:
  enabled: true
  service:
    type: LoadBalancer
    annotations: 
      io.cilium/lb-ipam-ips: "192.168.1.82"
    # REQUIRED: Label for the Cilium IP Pool selector
    labels:
      io.cilium/lb-ipam-ips: "true"

# IP .83: Peering Gateway (Cross-cluster Trust)
peering:
  enabled: true
# Note: Ensure your Peering through Mesh Gateway config is applied via a ConfigEntry

# IP .74: Shared Hub Gateway (UI/API Access)
connectInject:
  enabled: true
  apiGateway:
    managedGatewayClass:
      serviceType: LoadBalancer
      annotations:
        io.cilium/lb-ipam-ips: "192.168.1.74"
      # REQUIRED: Label for the Cilium IP Pool selector
      labels:
        io.cilium/lb-ipam-ips: "true"

apiVersion: ://consul.hashicorp.com
kind: Mesh
metadata:
  name: mesh
  namespace: consul
spec:
  peering:
    peerThroughMeshGateways: true




4. The Security Policy (cilium-policy.yaml)
To validate the Security Pillar, this cluster-wide policy allows the Gateway to talk to any tool you add later.
yaml
apiVersion: "cilium.io/v2"
kind: CiliumClusterwideNetworkPolicy
metadata:
  name: "well-architected-gateway-access"
spec:
  endpointSelector:
    matchExpressions:
      - {key: io.kubernetes.pod.namespace, operator: In, values: [consul, infra-system, monitoring]}
  ingress:
  - fromEndpoints:
    - matchLabels:
        cilium.io/gateway-name: "shared-hub-gateway"



apiVersion: "cilium.io/v2alpha1"
kind: CiliumLoadBalancerIPPool
metadata:
  name: consul-infra-pool
spec:
  blocks:
    - start: "192.168.1.74"
      stop: "192.168.1.74"  # Assigned to: Shared Hub (UIs)
    - start: "192.168.1.82"
      stop: "192.168.1.82"  # Assigned to: Mesh Gateway (Data)
    - start: "192.168.1.83"
      stop: "192.168.1.83"  # Assigned to: Peering Gateway (Trust)
    - start: "192.168.1.100"
      stop: "192.168.1.100" # Assigned to: xDS / gRPC (Control)

    serviceSelector:
      matchExpressions:
       - {key: io.cilium/lb-ipam-ips, operator: Exists} 
kubectl get nodes -o custom-columns=NAME:.metadata.name,EXCLUDE-LB:.metadata.labels."node\.kubernetes\.io/exclude-from-external-load-balancers"


5. Official 2026 Documentation for Maintenance
• Gateway API TLS Passthrough: Cilium Service Mesh Documentation.
• Multi-Cluster Peering: Consul 2026 Peering Specs.
• Talos BGP/Bird Setup: Talos Linux Network Documentation.
Why this is the Best Practice:
1. Scalability: When you add Grafana or Prometheus, you simply add a new HTTPRoute or add their hostname to the existing one. You don't touch the Gateway IP or BIRD.
2. No Certificate Conflicts: Consul manages its own encryption via Passthrough, and Rancher stays on plain HTTP (or its own certs). They never have to share.
3. Fault Isolation: If someone floods the Grafana UI, your Consul xDS (.244) and Mesh Data (.242) remain untouched.
