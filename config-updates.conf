To implement a Well-Architected 4-LB production setup for 2026, we will use Cilium Gateway API with Protocol Isolation. This allows Consul to use its internal TLS (via Passthrough) while Rancher, Grafana, and Prometheus use standard HTTP/HTTPS on the same shared IP.
1. The Multi-Purpose Gateway (gateway.yaml)
This file defines the Shared Hub (IP .240). It is architected to handle both encrypted traffic for Consul and plain traffic for other tools.


yaml
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: shared-hub-gateway
  namespace: infra-system
  annotations:
    io.cilium/lb-ipam-ips: "192.168.1.240"
spec:
  gatewayClassName: cilium
  listeners:
  - name: http
    protocol: HTTP
    port: 80
    allowedRoutes: { namespaces: { from: All } }
  - name: https-passthrough
    protocol: TLS
    port: 443
    tls:
      mode: Passthrough # Allows Consul to use its own TLS without sharing certs
    allowedRoutes: { namespaces: { from: All } }




2. The Application Routes (Cross-Namespace)
These files sit in the tool-specific namespaces and "attach" themselves to the Hub.
Consul (TLS Passthrough):
yaml
apiVersion: gateway.networking.k8s.io/v1alpha2
kind: TLSRoute
metadata:
  name: consul-ui-route
  namespace: consul
spec:
  parentRefs: [{ name: shared-hub-gateway, namespace: infra-system }]
  hostnames: ["consul.corp.local"]
  rules:
  - backendRefs: [{ name: consul-ui, port: 443 }]
Use o código com cuidado.
Rancher, Grafana, Prometheus (HTTP/HTTPS):
yaml
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: infra-tools-route
  namespace: infra-system
spec:
  parentRefs: [{ name: shared-hub-gateway, namespace: infra-system }]
  hostnames: ["rancher.corp.local", "grafana.corp.local", "prometheus.corp.local"]
  rules:
  - matches: [{ path: { type: PathPrefix, value: / } }]
    backendRefs:
    - name: rancher-service # Update these to your actual service names
      port: 80


3. Consolidated Consul Helm (values.yaml)
This configures the remaining 3 LBs: Control Plane (.244), Mesh (.242), and Peering (.243).
yaml
global:
  name: consul
  datacenter: central
  acls: { manageSystemACLs: true }
  tls: { enabled: true, enableAutoEncrypt: true }
# LB 2: xDS / gRPC
server:
  replicas: 3
  exposeGossipAndRPC:
    enabled: true
    type: LoadBalancer
    annotations: { io.cilium/lb-ipam-ips: "192.168.1.244" }
# LB 3: Mesh Gateway
meshGateway:
  enabled: true
  service:
    type: LoadBalancer
    annotations: { io.cilium/lb-ipam-ips: "192.168.1.242" }
# LB 4: Peering
peering: { enabled: true }
# Mesh Config for Peering
meshConfig:
  peering: { peerThroughMeshGateways: true }


4. The Security Policy (cilium-policy.yaml)
To validate the Security Pillar, this cluster-wide policy allows the Gateway to talk to any tool you add later.
yaml
apiVersion: "cilium.io/v2"
kind: CiliumClusterwideNetworkPolicy
metadata:
  name: "well-architected-gateway-access"
spec:
  endpointSelector:
    matchExpressions:
      - {key: io.kubernetes.pod.namespace, operator: In, values: [consul, infra-system, monitoring]}
  ingress:
  - fromEndpoints:
    - matchLabels:
        cilium.io/gateway-name: "shared-hub-gateway"

5. Official 2026 Documentation for Maintenance
• Gateway API TLS Passthrough: Cilium Service Mesh Documentation.
• Multi-Cluster Peering: Consul 2026 Peering Specs.
• Talos BGP/Bird Setup: Talos Linux Network Documentation.
Why this is the Best Practice:
1. Scalability: When you add Grafana or Prometheus, you simply add a new HTTPRoute or add their hostname to the existing one. You don't touch the Gateway IP or BIRD.
2. No Certificate Conflicts: Consul manages its own encryption via Passthrough, and Rancher stays on plain HTTP (or its own certs). They never have to share.
3. Fault Isolation: If someone floods the Grafana UI, your Consul xDS (.244) and Mesh Data (.242) remain untouched.
