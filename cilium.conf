helm upgrade cilium cilium/cilium \
  --namespace kube-system \
  --reuse-values \
  --set kubeProxyReplacement=strict \
  --set l7Proxy=true \
  --set envoy.enabled=true \
  --set bgpControlPlane.enabled=true \
  --set loadBalancer.mode=dsr \
  --set loadBalancer.acceleration=native \
  --set operator.replicas=2 \
  --set hubble.enabled=true \
  --set hubble.relay.enabled=true \
  --set hubble.ui.enabled=false \
  --set bpf.lbExternalClusterIP=true



MSYS_NO_PATHCONV=1 helm upgrade --install cilium cilium/cilium  
--namespace kube-system  --set loadBalancer.mode=ds --version 1.18.6   --set ipam.mode=kubernetes 
--set bpf.lbExternalClusterIP=true --set kubeProxyReplacement=true --set cgroup.autoMount.enabled=false 
--set cgroup.hostRoot=/sys/fs/cgroup   --set bgpControlPlane.enabled=true   --set l2announcements.enabled=true 
--set externalIPs.enabled=true   --set gatewayAPI.enabled=true  --set enable-host-reachable-services=true
--set nodePort.enabled=true
--set securityContext.capabilities.ciliumAgent="{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}" 
--set securityContext.capabilities.cleanCiliumState="{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}"   --set securityContext.allowPrivilegeEscalation=true 
--set l7Proxy=true --set envoy.enabled=true --set securityContext.seccompProfile.type=Unconfined --set loadBalancer.acceleration=native --set operator.replicas=2 
--set hubble.enabled=true --set hubble.relay.enabled=true --set bpf.hostLegacyRouting=true --set securityContext.runAsUser=0  --set hubble.ui.enabled=false


--------------------------------

apiVersion: v1
kind: Service
metadata:
  name: hub-ui-lb
  namespace: kube-system
  annotations:
    io.cilium/lb-ipam-ips: "192.168.1.70"
spec:
  type: LoadBalancer
  selector:
    app: cilium-envoy
  ports:
  - name: https
    port: 443
    targetPort: 443
-------------------------
apiVersion: v1
kind: Service
metadata:
  name: clusters-ui-lb
  namespace: kube-system
  annotations:
    io.cilium/lb-ipam-ips: "192.168.1.71"
spec:
  type: LoadBalancer
  selector:
    app: cilium-envoy
  ports:
  - name: https
    port: 443
    targetPort: 443
------------------

apiVersion: cilium.io/v2
kind: CiliumEnvoyConfig
metadata:
  name: ui-routing
  namespace: kube-system
spec:
  services:
  - name: hub-ui-lb
  - name: clusters-ui-lb
  listeners:
  - name: https
    address: 0.0.0.0
    port: 443
    filterChains:
    - filterChainMatch:
        serverNames: ["*.hub.example.com"]
      filters:
      - name: envoy.filters.network.http_connection_manager
        typedConfig:
          routeConfig:
            virtualHosts:
            - name: hub
              domains: ["*.hub.example.com"]
              routes:
              - match: { prefix: "/" }
                route:
                  cluster: hub-services

    - filterChainMatch:
        serverNames: ["*.cluster.example.com"]
      filters:
      - name: envoy.filters.network.http_connection_manager
        typedConfig:
          routeConfig:
            virtualHosts:
            - name: remote
              domains: ["*.cluster.example.com"]
              routes:
              - match: { prefix: "/" }
                route:
                  cluster: mesh-gateway
------------------------------------------------

node:
  id: envoy-edge-1
  cluster: global-edge

dynamic_resources:
  ads_config:
    api_type: GRPC
    grpc_services:
    - envoy_grpc:
        cluster_name: consul-xds

static_resources:
  clusters:
  - name: consul-xds
    type: logical_dns
    connect_timeout: 5s
    http2_protocol_options: {}
    load_assignment:
      endpoints:
      - lb_endpoints:
        - endpoint:
            address:
              socket_address:
                address: 192.168.1.81
                port_value: 8502

#########################################################


User / Internet / Corp
        |
        | DNS (*.cluster.com)
        v
 +------------------+
 | Envoy VM (x2)    |  <-- GLOBAL EDGE
 | (Consul xDS)     |
 +------------------+
        |
        | VIP targets (not pods)
        v
 +------------------+
 | BGP BIRD2 VM     |  <-- GLOBAL EDGE
 | (Consul xDS)     |
 +------------------+
   |
        | VIP targets (not pods)
        v
 +-----------------------+
 | Cilium L7 (Hub VIPs)  |  <-- K8s Edge
 +-----------------------+
        |
        | SNI routing
        v
 +--------------------------+
 | Hub svc OR               |
 | Mesh Gateway â†’ cluster X |
 +--------------------------+

--------------------------------------------------------------------




                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Internet /  â”‚
                    â”‚ Corporate DNS â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                    DNS â†’ two FQDN patterns
                            â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                           â”‚
     VIP #1 (Hub UI)              VIP #2 (Cluster UIs)
     192.168.1.70                 192.168.1.71
     (Bird2 â†’ Cilium)             (Bird2 â†’ Cilium)
              â”‚                           â”‚
        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
        â”‚ Cilium L7 â”‚               â”‚ Cilium L7 â”‚
        â”‚ Envoy     â”‚               â”‚ Envoy     â”‚
        â”‚ (Hub only)â”‚               â”‚ (Remote)  â”‚
        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
              â”‚                           â”‚
   Hub services (local)          Consul Mesh Gateways
   Rancher / Grafana             (per cluster)
   Consul UI                     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Spoke clusters
                                               (GitLab, UIs, apps)

CONTROL PLANE (separate, stable):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2Ã— Envoy VMs (Proxmox)                   â”‚
â”‚ - Consul XDS (gRPC 8502)                 â”‚
â”‚ - Admin UI (shared)                      â”‚
â”‚ - NO data-plane traffic                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
-------------------------------------------------------




Cluster A user / CI / agent
        |
        | DNS â†’ gitlab.cluster-b.clusters.example.com
        v
BGP VIP (192.168.1.71)  â† advertised by Bird2
        |
        v
Cilium L7 (Hub)
        |
        | ALL *.clusters.example.com
        v
Consul Mesh Gateway (Hub)
        |
        | mTLS + service identity
        v
Mesh Gateway (Cluster B)
        |
        v
GitLab Service (Cluster B)

------------------------------------------
peering 
 Normal operation

            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   DNS      â”‚
            â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Bird2 (Site A) â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ VIPs
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Hub A (Cilium +    â”‚
        â”‚ Consul)            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Remote clusters â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
------------------------------------


Hub failure â†’ automatic failover

            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   DNS      â”‚
            â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Bird2 (Site B) â”‚  â† iBGP / eBGP
          â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ VIPs
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Hub B (Replica)    â”‚
        â”‚ (same config)      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Remote clusters â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
######################################################################################

prod minimal 
LoadBalancers routing traffic  (advertised by Bird2)
apiVersion: v1
kind: Service
metadata:
  name: hub-ui-lb
  namespace: kube-system
  annotations:
    io.cilium/lb-ipam-ips: "192.168.1.70"
spec:
  type: LoadBalancer
  selector:
    app: cilium-envoy
  ports:
  - name: https
    port: 443
    targetPort: 443
---
apiVersion: v1
kind: Service
metadata:
  name: clusters-ui-lb
  namespace: kube-system
  annotations:
    io.cilium/lb-ipam-ips: "192.168.1.71"
spec:
  type: LoadBalancer
  selector:
    app: cilium-envoy
  ports:
  - name: https
    port: 443
    targetPort: 443

âœ” Bird2 advertises these
âœ” DNS points to these
âœ” They land on Cilium Envoy pods
---
Cilium L7 (single Envoy config, both HUB + clusters)

apiVersion: cilium.io/v2
kind: CiliumEnvoyConfig
metadata:
  name: global-ui-ingress
  namespace: kube-system
spec:
  services:
    - name: hub-ui-lb
    - name: clusters-ui-lb

  listeners:
  - name: https
    address: 0.0.0.0
    port: 443

    filterChains:

    # ===== HUB UIs =====
    - filterChainMatch:
        serverNames:
          - "consul.hub.example.com"
          - "rancher.hub.example.com"
          - "grafana.hub.example.com"
      filters:
      - name: envoy.filters.network.http_connection_manager
        typedConfig:
          stat_prefix: hub_https
          routeConfig:
            virtualHosts:
            - name: hub
              domains:
                - "consul.hub.example.com"
                - "rancher.hub.example.com"
                - "grafana.hub.example.com"
              routes:
              - match: { prefix: "/" }
                route:
                  cluster: hub-services

    # ===== ALL REMOTE CLUSTERS =====
    - filterChainMatch:
        serverNames:
          - "*.clusters.example.com"
      filters:
      - name: envoy.filters.network.http_connection_manager
        typedConfig:
          stat_prefix: remote_https
          routeConfig:
            virtualHosts:
            - name: remote
              domains:
                - "*.clusters.example.com"
              routes:
              - match: { prefix: "/" }
                route:
                  cluster: consul-mesh-gateway

ðŸ”‘ THIS is the scaling trick
Only two clusters defined:

hub-services

consul-mesh-gateway
---

Upstream definitions (where traffic goes)

apiVersion: cilium.io/v2
kind: CiliumEnvoyConfig
metadata:
  name: global-ui-upstreams
  namespace: kube-system
spec:
  clusters:
  - name: hub-services
    type: logical_dns
    connect_timeout: 5s
    lb_policy: round_robin
    load_assignment:
      cluster_name: hub-services
      endpoints:
      - lb_endpoints:
        - endpoint:
            address:
              socket_address:
                address: hub-ui.platform.svc.cluster.local
                port_value: 443

  - name: consul-mesh-gateway
    type: logical_dns
    connect_timeout: 5s
    lb_policy: round_robin
    load_assignment:
      cluster_name: consul-mesh-gateway
      endpoints:
      - lb_endpoints:
        - endpoint:
            address:
              socket_address:
                address: mesh-gateway.consul.svc.cluster.local
                port_value: 8443

âœ” Cilium only forwards
âœ” Consul decides destination cluster
---

apiVersion: v1
kind: Service
metadata:
  name: hub-ui
  namespace: platform
spec:
  selector:
    app: platform-ui
  ports:
  - port: 443

This service can front:
Rancher
Consul UI
Grafana
(via internal routing, ingress, or separate vhosts)
-------


#$$$$$$$$$$$$$$$$$$$#$#$#$#$############################################

his is a very good call ðŸ‘ â€” and youâ€™re right about why it feels better.

Short answer up front:

âœ… Yes, you can (and should) move to Cilium Gateway API

âœ… No, it will not conflict if you do NOT install Consul API Gateway

âœ… Gateway API is the modern, CNCF / cloud-aligned way

âœ… It drastically simplifies config vs raw CiliumEnvoyConfig

What you were feeling (â€œtoo much config, hard to updateâ€) is exactly why Gateway API exists.

Letâ€™s go step by step and make this production-ready and clear.

1ï¸âƒ£ Will Cilium Gateway API conflict with Consul?
âŒ What causes conflicts

Conflicts only happen if you install:

Consul API Gateway

AND Cilium Gateway API

Because both try to:

manage Envoy

bind ports

own L7 ingress

âœ… Your situation (SAFE)

You said:

â€œI am not installing api gateway anymore in the clusterâ€

That means:

Component	Role
Cilium Gateway API	L7 ingress (HTTP/SNI routing)
Consul Mesh Gateway	Cross-cluster routing
Consul xDS	Control plane (for Envoy VMs, future use)

âœ” No overlap
âœ” No double Envoy
âœ” No CRD conflict

This is the recommended split.

2ï¸âƒ£ Why Gateway API is better than raw Cilium L7

You already noticed the pain. Hereâ€™s the concrete comparison:

âŒ CiliumEnvoyConfig (old way)

Envoy-native

Verbose

Hard to review

Hard to update

Easy to break

Not portable

âœ… Gateway API (new way)

Kubernetes-native

Declarative

Short YAML

Clear ownership

CNCF standard

Cloud-like (AWS / GCP / Azure)

Youâ€™re aligning with:

CNCF Gateway API

AWS ALB / GKE Gateway patterns

Well-Architected Framework

3ï¸âƒ£ Your FINAL architecture (updated, clean)

Nothing changes conceptually â€” only how L7 rules are expressed.

Client / Cluster / Internet
        |
        | DNS (*.example.com)
        v
Bird2 VIP
        |
        v
Cilium Gateway (Gateway API)
        |
        | Host-based routing
        v
+-----------------------------+
| hub services  OR            |
| Consul Mesh Gateway (HUB)   |
+-----------------------------+
        |
        v
Remote cluster Mesh Gateway
        |
        v
Service (GitLab, etc)


Still:

2 VIPs

1 hub ingress

Unlimited clusters

No per-cluster IPs

4ï¸âƒ£ What you need to enable in Cilium (Helm)

This is the only required change.

helm upgrade cilium cilium/cilium \
  --namespace kube-system \
  --reuse-values \
  --set gatewayAPI.enabled=true \
  --set envoy.enabled=true \
  --set l7Proxy=true


Thatâ€™s it.

âœ” No Consul API Gateway
âœ” No extra controllers
âœ” Cilium owns Gateway API

5ï¸âƒ£ PRODUCTION Gateway API config (HUB)
5.1 GatewayClass (once)
apiVersion: gateway.networking.k8s.io/v1
kind: GatewayClass
metadata:
  name: cilium
spec:
  controllerName: io.cilium/gateway-controller

5.2 Gateways (2 VIPs, isolated)
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: hub-ui-gateway
  namespace: kube-system
spec:
  gatewayClassName: cilium
  listeners:
  - name: https
    port: 443
    protocol: HTTPS
    hostname: "*.hub.example.com"
    tls:
      mode: Passthrough
---
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: clusters-ui-gateway
  namespace: kube-system
spec:
  gatewayClassName: cilium
  listeners:
  - name: https
    port: 443
    protocol: HTTPS
    hostname: "*.clusters.example.com"
    tls:
      mode: Passthrough


VIPs are allocated automatically via Cilium LB IPAM
(or you can pin IPs via annotations if you want)

6ï¸âƒ£ Routes (THIS is where it becomes simple)
6.1 Hub services (Rancher, Consul UI, Grafana)
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: hub-ui
  namespace: platform
spec:
  parentRefs:
  - name: hub-ui-gateway
    namespace: kube-system
  hostnames:
  - "rancher.hub.example.com"
  - "consul.hub.example.com"
  - "grafana.hub.example.com"
  rules:
  - backendRefs:
    - name: hub-ui
      port: 443


Thatâ€™s it.
No Envoy clusters.
No filter chains.
Readable.

6.2 ALL remote clusters (single rule)
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: remote-clusters
  namespace: consul
spec:
  parentRefs:
  - name: clusters-ui-gateway
    namespace: kube-system
  hostnames:
  - "*.clusters.example.com"
  rules:
  - backendRefs:
    - name: mesh-gateway
      port: 8443


ðŸ”‘ This one rule enables infinite clusters

7ï¸âƒ£ What EACH other cluster still needs (unchanged)

Gateway API does not change this part.

Each cluster installs:

Consul client

Mesh Gateway

Service registration

Thatâ€™s all.

They:

do NOT expose ingress

do NOT need VIPs

do NOT need Gateway API

8ï¸âƒ£ Failure & scale behavior (unchanged, but cleaner)

Gateway API objects are reconciled automatically

Cilium handles Envoy lifecycle

No YAML edits for new clusters

No reloads

Same BGP failover story

9ï¸âƒ£ Final recommendation (clear & confident)

You are making the right choice.

âœ… Use Cilium Gateway API
âŒ Do not use Consul API Gateway
âœ… Keep Consul Mesh Gateway
âœ… Keep Envoy VMs for xDS / future edge
âœ… Keep Bird2 for VIPs

This is:

modern

simpler

safer

cloud-aligned

production-grade

If you want next, I can:

convert your exact current L7 rules to Gateway API

show TLS termination vs passthrough choices

add auth (OIDC) at Gateway API

show blue/green hub failover

##########################################


register clients 
how the full client config for each cluster should look like 
global:
  name: "spoke-42"                 # UNIQUE cluster name
  datacenter: "dc1"                # Same DC is OK for OSS
  domain: consul
  image: "hashicorp/consul:1.16.x"

  # IMPORTANT: point ONLY to the HUB CONSUL SERVER VIP
  retry_join:
    - "192.168.1.10"               # consul-server LoadBalancer VIP

  tls:
    enabled: true
    enableAutoEncrypt: true
    verify: true

server:
  enabled: false                   # SPOKES NEVER RUN SERVERS

client:
  enabled: true
  grpc: true                       # REQUIRED for mesh & xDS
  exposeGossipPorts: false

connectInject:
  enabled: true
  default: false                   # opt-in per namespace

meshGateway:
  enabled: true
  mode: local                      # spoke-local gateway
  replicas: 1

ui:
  enabled: false                   # UI ONLY IN HUB

dns:
  enabled: true
  enableRedirection: true
  requestTimeout: "5s"

syncCatalog:
  enabled: true
  toConsul: true
  fromConsul: false                # hub is source of truth


#
helm upgrade --install consul hashicorp/consul \
  --namespace consul \
  --values maqa-spoke.yaml \
  --atomic \
  --timeout 5m
Flag --atomic has been deprecated, use --rollback-on-failure instead
level=WARN msg="unable to find exact version; falling back to closest available version" chart=consul requested="" selected=1.9.2
I0126 17:26:53.254000   22504 warnings.go:110] "Warning: would violate PodSecurity \"restricted:latest\": hostPort (container \"consul\" uses hostPorts 8501, 8502), allowPrivilegeEscalation != false (container \"consul\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"consul\" must set securityContext.capabilities.drop=[\"ALL\"]), seccompProfile (pod or container \"consul\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")"
level=WARN msg="upgrade failed" name=consul error="resource not ready, name: mas-qa-client, kind: DaemonSet, status: InProgress\nresource not ready, name: mas-qa-connect-injector, kind: Deployment, status: InProgress\nresource not ready, name: mas-qa-mesh-gateway, kind: Deployment, status: InProgress\nresource not ready, name: mas-qa-sync-catalog, kind: Deployment, status: InProgress\ncontext deadline exceeded"
I0126 17:32:11.620276   22504 warnings.go:110] "Warning: would violate PodSecurity \"restricted:latest\": hostPort (container \"consul\" uses hostPorts 8500, 8502), allowPrivilegeEscalation != false (container \"consul\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"consul\" must set securityContext.capabilities.drop=[\"ALL\"]), seccompProfile (pod or container \"consul\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")"
Error: UPGRADE FAILED: an error occurred while rolling back the release. original upgrade error: resource not ready, name: mas-qa-client, kind: DaemonSet, status: InProgress
resource not ready, name: mas-qa-connect-injector, kind: Deployment, status: InProgress
resource not ready, name: mas-qa-mesh-gateway, kind: Deployment, status: InProgress
resource not ready, name: mas-qa-sync-catalog, kind: Deployment, status: InProgress
context deadline exceeded: release consul failed: resource not ready, name: consul-client, kind: DaemonSet, status: InProgress
resource not ready, name: consul-connect-injector, kind: Deployment, status: InProgress
context deadline exceeded


####
securityContext:
    allowPrivilegeEscalation: false
    runAsNonRoot: true
    seccompProfile:
      type: RuntimeDefault
    capabilities:
      drop:
        - ALL

