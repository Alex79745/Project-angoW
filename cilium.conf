helm upgrade cilium cilium/cilium \
  --namespace kube-system \
  --reuse-values \
  --set kubeProxyReplacement=strict \
  --set l7Proxy=true \
  --set envoy.enabled=true \
  --set bgpControlPlane.enabled=true \
  --set loadBalancer.mode=dsr \
  --set loadBalancer.acceleration=native \
  --set operator.replicas=2 \
  --set hubble.enabled=true \
  --set hubble.relay.enabled=true \
  --set hubble.ui.enabled=false \
  --set bpf.lbExternalClusterIP=true



MSYS_NO_PATHCONV=1 helm upgrade --install cilium cilium/cilium  
--namespace kube-system  --set loadBalancer.mode=ds --version 1.18.6   --set ipam.mode=kubernetes 
--set bpf.lbExternalClusterIP=true --set kubeProxyReplacement=true --set cgroup.autoMount.enabled=false 
--set cgroup.hostRoot=/sys/fs/cgroup   --set bgpControlPlane.enabled=true   --set l2announcements.enabled=true 
--set externalIPs.enabled=true   --set gatewayAPI.enabled=true  --set enable-host-reachable-services=true
--set nodePort.enabled=true
--set securityContext.capabilities.ciliumAgent="{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}" 
--set securityContext.capabilities.cleanCiliumState="{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}"   --set securityContext.allowPrivilegeEscalation=true 
--set l7Proxy=true --set envoy.enabled=true --set securityContext.seccompProfile.type=Unconfined --set loadBalancer.acceleration=native --set operator.replicas=2 
--set hubble.enabled=true --set hubble.relay.enabled=true --set bpf.hostLegacyRouting=true --set securityContext.runAsUser=0  --set hubble.ui.enabled=false


--------------------------------

apiVersion: v1
kind: Service
metadata:
  name: hub-ui-lb
  namespace: kube-system
  annotations:
    io.cilium/lb-ipam-ips: "192.168.1.70"
spec:
  type: LoadBalancer
  selector:
    app: cilium-envoy
  ports:
  - name: https
    port: 443
    targetPort: 443
-------------------------
apiVersion: v1
kind: Service
metadata:
  name: clusters-ui-lb
  namespace: kube-system
  annotations:
    io.cilium/lb-ipam-ips: "192.168.1.71"
spec:
  type: LoadBalancer
  selector:
    app: cilium-envoy
  ports:
  - name: https
    port: 443
    targetPort: 443
------------------

apiVersion: cilium.io/v2
kind: CiliumEnvoyConfig
metadata:
  name: ui-routing
  namespace: kube-system
spec:
  services:
  - name: hub-ui-lb
  - name: clusters-ui-lb
  listeners:
  - name: https
    address: 0.0.0.0
    port: 443
    filterChains:
    - filterChainMatch:
        serverNames: ["*.hub.example.com"]
      filters:
      - name: envoy.filters.network.http_connection_manager
        typedConfig:
          routeConfig:
            virtualHosts:
            - name: hub
              domains: ["*.hub.example.com"]
              routes:
              - match: { prefix: "/" }
                route:
                  cluster: hub-services

    - filterChainMatch:
        serverNames: ["*.cluster.example.com"]
      filters:
      - name: envoy.filters.network.http_connection_manager
        typedConfig:
          routeConfig:
            virtualHosts:
            - name: remote
              domains: ["*.cluster.example.com"]
              routes:
              - match: { prefix: "/" }
                route:
                  cluster: mesh-gateway
------------------------------------------------

node:
  id: envoy-edge-1
  cluster: global-edge

dynamic_resources:
  ads_config:
    api_type: GRPC
    grpc_services:
    - envoy_grpc:
        cluster_name: consul-xds

static_resources:
  clusters:
  - name: consul-xds
    type: logical_dns
    connect_timeout: 5s
    http2_protocol_options: {}
    load_assignment:
      endpoints:
      - lb_endpoints:
        - endpoint:
            address:
              socket_address:
                address: 192.168.1.81
                port_value: 8502

#########################################################


User / Internet / Corp
        |
        | DNS (*.cluster.com)
        v
 +------------------+
 | Envoy VM (x2)    |  <-- GLOBAL EDGE
 | (Consul xDS)     |
 +------------------+
        |
        | VIP targets (not pods)
        v
 +------------------+
 | BGP BIRD2 VM     |  <-- GLOBAL EDGE
 | (Consul xDS)     |
 +------------------+
   |
        | VIP targets (not pods)
        v
 +-----------------------+
 | Cilium L7 (Hub VIPs)  |  <-- K8s Edge
 +-----------------------+
        |
        | SNI routing
        v
 +--------------------------+
 | Hub svc OR               |
 | Mesh Gateway â†’ cluster X |
 +--------------------------+

--------------------------------------------------------------------




                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Internet /  â”‚
                    â”‚ Corporate DNS â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                    DNS â†’ two FQDN patterns
                            â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                           â”‚
     VIP #1 (Hub UI)              VIP #2 (Cluster UIs)
     192.168.1.70                 192.168.1.71
     (Bird2 â†’ Cilium)             (Bird2 â†’ Cilium)
              â”‚                           â”‚
        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
        â”‚ Cilium L7 â”‚               â”‚ Cilium L7 â”‚
        â”‚ Envoy     â”‚               â”‚ Envoy     â”‚
        â”‚ (Hub only)â”‚               â”‚ (Remote)  â”‚
        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
              â”‚                           â”‚
   Hub services (local)          Consul Mesh Gateways
   Rancher / Grafana             (per cluster)
   Consul UI                     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Spoke clusters
                                               (GitLab, UIs, apps)

CONTROL PLANE (separate, stable):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2Ã— Envoy VMs (Proxmox)                   â”‚
â”‚ - Consul XDS (gRPC 8502)                 â”‚
â”‚ - Admin UI (shared)                      â”‚
â”‚ - NO data-plane traffic                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
-------------------------------------------------------




Cluster A user / CI / agent
        |
        | DNS â†’ gitlab.cluster-b.clusters.example.com
        v
BGP VIP (192.168.1.71)  â† advertised by Bird2
        |
        v
Cilium L7 (Hub)
        |
        | ALL *.clusters.example.com
        v
Consul Mesh Gateway (Hub)
        |
        | mTLS + service identity
        v
Mesh Gateway (Cluster B)
        |
        v
GitLab Service (Cluster B)

------------------------------------------
peering 
 Normal operation

            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   DNS      â”‚
            â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Bird2 (Site A) â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ VIPs
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Hub A (Cilium +    â”‚
        â”‚ Consul)            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Remote clusters â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
------------------------------------


Hub failure â†’ automatic failover

            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   DNS      â”‚
            â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Bird2 (Site B) â”‚  â† iBGP / eBGP
          â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ VIPs
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Hub B (Replica)    â”‚
        â”‚ (same config)      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Remote clusters â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
######################################################################################

prod minimal 
LoadBalancers routing traffic  (advertised by Bird2)
apiVersion: v1
kind: Service
metadata:
  name: hub-ui-lb
  namespace: kube-system
  annotations:
    io.cilium/lb-ipam-ips: "192.168.1.70"
spec:
  type: LoadBalancer
  selector:
    app: cilium-envoy
  ports:
  - name: https
    port: 443
    targetPort: 443
---
apiVersion: v1
kind: Service
metadata:
  name: clusters-ui-lb
  namespace: kube-system
  annotations:
    io.cilium/lb-ipam-ips: "192.168.1.71"
spec:
  type: LoadBalancer
  selector:
    app: cilium-envoy
  ports:
  - name: https
    port: 443
    targetPort: 443

âœ” Bird2 advertises these
âœ” DNS points to these
âœ” They land on Cilium Envoy pods
---
Cilium L7 (single Envoy config, both HUB + clusters)

apiVersion: cilium.io/v2
kind: CiliumEnvoyConfig
metadata:
  name: global-ui-ingress
  namespace: kube-system
spec:
  services:
    - name: hub-ui-lb
    - name: clusters-ui-lb

  listeners:
  - name: https
    address: 0.0.0.0
    port: 443

    filterChains:

    # ===== HUB UIs =====
    - filterChainMatch:
        serverNames:
          - "consul.hub.example.com"
          - "rancher.hub.example.com"
          - "grafana.hub.example.com"
      filters:
      - name: envoy.filters.network.http_connection_manager
        typedConfig:
          stat_prefix: hub_https
          routeConfig:
            virtualHosts:
            - name: hub
              domains:
                - "consul.hub.example.com"
                - "rancher.hub.example.com"
                - "grafana.hub.example.com"
              routes:
              - match: { prefix: "/" }
                route:
                  cluster: hub-services

    # ===== ALL REMOTE CLUSTERS =====
    - filterChainMatch:
        serverNames:
          - "*.clusters.example.com"
      filters:
      - name: envoy.filters.network.http_connection_manager
        typedConfig:
          stat_prefix: remote_https
          routeConfig:
            virtualHosts:
            - name: remote
              domains:
                - "*.clusters.example.com"
              routes:
              - match: { prefix: "/" }
                route:
                  cluster: consul-mesh-gateway

ðŸ”‘ THIS is the scaling trick
Only two clusters defined:

hub-services

consul-mesh-gateway
---

Upstream definitions (where traffic goes)

apiVersion: cilium.io/v2
kind: CiliumEnvoyConfig
metadata:
  name: global-ui-upstreams
  namespace: kube-system
spec:
  clusters:
  - name: hub-services
    type: logical_dns
    connect_timeout: 5s
    lb_policy: round_robin
    load_assignment:
      cluster_name: hub-services
      endpoints:
      - lb_endpoints:
        - endpoint:
            address:
              socket_address:
                address: hub-ui.platform.svc.cluster.local
                port_value: 443

  - name: consul-mesh-gateway
    type: logical_dns
    connect_timeout: 5s
    lb_policy: round_robin
    load_assignment:
      cluster_name: consul-mesh-gateway
      endpoints:
      - lb_endpoints:
        - endpoint:
            address:
              socket_address:
                address: mesh-gateway.consul.svc.cluster.local
                port_value: 8443

âœ” Cilium only forwards
âœ” Consul decides destination cluster
---

apiVersion: v1
kind: Service
metadata:
  name: hub-ui
  namespace: platform
spec:
  selector:
    app: platform-ui
  ports:
  - port: 443

This service can front:
Rancher
Consul UI
Grafana
(via internal routing, ingress, or separate vhosts)
-------


#$$$$$$$$$$$$$$$$$$$#$#$#$#$############################################

his is a very good call ðŸ‘ â€” and youâ€™re right about why it feels better.

Short answer up front:

âœ… Yes, you can (and should) move to Cilium Gateway API

âœ… No, it will not conflict if you do NOT install Consul API Gateway

âœ… Gateway API is the modern, CNCF / cloud-aligned way

âœ… It drastically simplifies config vs raw CiliumEnvoyConfig

What you were feeling (â€œtoo much config, hard to updateâ€) is exactly why Gateway API exists.

Letâ€™s go step by step and make this production-ready and clear.

1ï¸âƒ£ Will Cilium Gateway API conflict with Consul?
âŒ What causes conflicts

Conflicts only happen if you install:

Consul API Gateway

AND Cilium Gateway API

Because both try to:

manage Envoy

bind ports

own L7 ingress

âœ… Your situation (SAFE)

You said:

â€œI am not installing api gateway anymore in the clusterâ€

That means:

Component	Role
Cilium Gateway API	L7 ingress (HTTP/SNI routing)
Consul Mesh Gateway	Cross-cluster routing
Consul xDS	Control plane (for Envoy VMs, future use)

âœ” No overlap
âœ” No double Envoy
âœ” No CRD conflict

This is the recommended split.

2ï¸âƒ£ Why Gateway API is better than raw Cilium L7

You already noticed the pain. Hereâ€™s the concrete comparison:

âŒ CiliumEnvoyConfig (old way)

Envoy-native

Verbose

Hard to review

Hard to update

Easy to break

Not portable

âœ… Gateway API (new way)

Kubernetes-native

Declarative

Short YAML

Clear ownership

CNCF standard

Cloud-like (AWS / GCP / Azure)

Youâ€™re aligning with:

CNCF Gateway API

AWS ALB / GKE Gateway patterns

Well-Architected Framework

3ï¸âƒ£ Your FINAL architecture (updated, clean)

Nothing changes conceptually â€” only how L7 rules are expressed.

Client / Cluster / Internet
        |
        | DNS (*.example.com)
        v
Bird2 VIP
        |
        v
Cilium Gateway (Gateway API)
        |
        | Host-based routing
        v
+-----------------------------+
| hub services  OR            |
| Consul Mesh Gateway (HUB)   |
+-----------------------------+
        |
        v
Remote cluster Mesh Gateway
        |
        v
Service (GitLab, etc)


Still:

2 VIPs

1 hub ingress

Unlimited clusters

No per-cluster IPs

4ï¸âƒ£ What you need to enable in Cilium (Helm)

This is the only required change.

helm upgrade cilium cilium/cilium \
  --namespace kube-system \
  --reuse-values \
  --set gatewayAPI.enabled=true \
  --set envoy.enabled=true \
  --set l7Proxy=true


Thatâ€™s it.

âœ” No Consul API Gateway
âœ” No extra controllers
âœ” Cilium owns Gateway API

5ï¸âƒ£ PRODUCTION Gateway API config (HUB)
5.1 GatewayClass (once)
apiVersion: gateway.networking.k8s.io/v1
kind: GatewayClass
metadata:
  name: cilium
spec:
  controllerName: io.cilium/gateway-controller

5.2 Gateways (2 VIPs, isolated)
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: hub-ui-gateway
  namespace: kube-system
spec:
  gatewayClassName: cilium
  listeners:
  - name: https
    port: 443
    protocol: HTTPS
    hostname: "*.hub.example.com"
    tls:
      mode: Passthrough
---
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: clusters-ui-gateway
  namespace: kube-system
spec:
  gatewayClassName: cilium
  listeners:
  - name: https
    port: 443
    protocol: HTTPS
    hostname: "*.clusters.example.com"
    tls:
      mode: Passthrough


VIPs are allocated automatically via Cilium LB IPAM
(or you can pin IPs via annotations if you want)

6ï¸âƒ£ Routes (THIS is where it becomes simple)
6.1 Hub services (Rancher, Consul UI, Grafana)
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: hub-ui
  namespace: platform
spec:
  parentRefs:
  - name: hub-ui-gateway
    namespace: kube-system
  hostnames:
  - "rancher.hub.example.com"
  - "consul.hub.example.com"
  - "grafana.hub.example.com"
  rules:
  - backendRefs:
    - name: hub-ui
      port: 443


Thatâ€™s it.
No Envoy clusters.
No filter chains.
Readable.

6.2 ALL remote clusters (single rule)
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: remote-clusters
  namespace: consul
spec:
  parentRefs:
  - name: clusters-ui-gateway
    namespace: kube-system
  hostnames:
  - "*.clusters.example.com"
  rules:
  - backendRefs:
    - name: mesh-gateway
      port: 8443


ðŸ”‘ This one rule enables infinite clusters

7ï¸âƒ£ What EACH other cluster still needs (unchanged)

Gateway API does not change this part.

Each cluster installs:

Consul client

Mesh Gateway

Service registration

Thatâ€™s all.

They:

do NOT expose ingress

do NOT need VIPs

do NOT need Gateway API

8ï¸âƒ£ Failure & scale behavior (unchanged, but cleaner)

Gateway API objects are reconciled automatically

Cilium handles Envoy lifecycle

No YAML edits for new clusters

No reloads

Same BGP failover story

9ï¸âƒ£ Final recommendation (clear & confident)

You are making the right choice.

âœ… Use Cilium Gateway API
âŒ Do not use Consul API Gateway
âœ… Keep Consul Mesh Gateway
âœ… Keep Envoy VMs for xDS / future edge
âœ… Keep Bird2 for VIPs

This is:

modern

simpler

safer

cloud-aligned

production-grade

If you want next, I can:

convert your exact current L7 rules to Gateway API

show TLS termination vs passthrough choices

add auth (OIDC) at Gateway API

show blue/green hub failover

##########################################


register clients 
how the full client config for each cluster should look like 
global:
  name: "spoke-42"                 # UNIQUE cluster name
  datacenter: "dc1"                # Same DC is OK for OSS
  domain: consul
  image: "hashicorp/consul:1.16.x"

  # IMPORTANT: point ONLY to the HUB CONSUL SERVER VIP
  retry_join:
    - "192.168.1.10"               # consul-server LoadBalancer VIP

  tls:
    enabled: true
    enableAutoEncrypt: true
    verify: true

server:
  enabled: false                   # SPOKES NEVER RUN SERVERS

client:
  enabled: true
  grpc: true                       # REQUIRED for mesh & xDS
  exposeGossipPorts: false

connectInject:
  enabled: true
  default: false                   # opt-in per namespace

meshGateway:
  enabled: true
  mode: local                      # spoke-local gateway
  replicas: 1

ui:
  enabled: false                   # UI ONLY IN HUB

dns:
  enabled: true
  enableRedirection: true
  requestTimeout: "5s"

syncCatalog:
  enabled: true
  toConsul: true
  fromConsul: false                # hub is source of truth


#
helm upgrade --install consul hashicorp/consul \
  --namespace consul \
  --values maqa-spoke.yaml \
  --atomic \
  --timeout 5m
Flag --atomic has been deprecated, use --rollback-on-failure instead
level=WARN msg="unable to find exact version; falling back to closest available version" chart=consul requested="" selected=1.9.2
I0126 17:26:53.254000   22504 warnings.go:110] "Warning: would violate PodSecurity \"restricted:latest\": hostPort (container \"consul\" uses hostPorts 8501, 8502), allowPrivilegeEscalation != false (container \"consul\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"consul\" must set securityContext.capabilities.drop=[\"ALL\"]), seccompProfile (pod or container \"consul\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")"
level=WARN msg="upgrade failed" name=consul error="resource not ready, name: mas-qa-client, kind: DaemonSet, status: InProgress\nresource not ready, name: mas-qa-connect-injector, kind: Deployment, status: InProgress\nresource not ready, name: mas-qa-mesh-gateway, kind: Deployment, status: InProgress\nresource not ready, name: mas-qa-sync-catalog, kind: Deployment, status: InProgress\ncontext deadline exceeded"
I0126 17:32:11.620276   22504 warnings.go:110] "Warning: would violate PodSecurity \"restricted:latest\": hostPort (container \"consul\" uses hostPorts 8500, 8502), allowPrivilegeEscalation != false (container \"consul\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"consul\" must set securityContext.capabilities.drop=[\"ALL\"]), seccompProfile (pod or container \"consul\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")"
Error: UPGRADE FAILED: an error occurred while rolling back the release. original upgrade error: resource not ready, name: mas-qa-client, kind: DaemonSet, status: InProgress
resource not ready, name: mas-qa-connect-injector, kind: Deployment, status: InProgress
resource not ready, name: mas-qa-mesh-gateway, kind: Deployment, status: InProgress
resource not ready, name: mas-qa-sync-catalog, kind: Deployment, status: InProgress
context deadline exceeded: release consul failed: resource not ready, name: consul-client, kind: DaemonSet, status: InProgress
resource not ready, name: consul-connect-injector, kind: Deployment, status: InProgress
context deadline exceeded


####
securityContext:
    allowPrivilegeEscalation: false
    runAsNonRoot: true
    seccompProfile:
      type: RuntimeDefault
    capabilities:
      drop:
        - ALL

 helm upgrade --install consul hashicorp/consul   --namespace consul   --values maqa-spoke.yaml   --rollback-on-failure   --timeout 5m                                     level=WARN msg="unable to find exact version; falling back to closest available version" chart=consul requested="" selected=1.9.2
level=WARN msg="upgrade failed" name=consul error="resource not ready, name: mas-qa-client, kind: DaemonSet, status: InProgress\nresource not ready, name: mas-qa-connect-injector, kind: Deployment, status: InProgress\nresource not ready, name: mas-qa-mesh-gateway, kind: Deployment, status: InProgress\nresource not ready, name: mas-qa-sync-catalog, kind: Deployment, status: InProgress\ncontext deadline exceeded"
Error: UPGRADE FAILED: an error occurred while rolling back the release. original upgrade error: resource not ready, name: mas-qa-client, kind: DaemonSet, status: InProgress
resource not ready, name: mas-qa-connect-injector, kind: Deployment, status: InProgress
resource not ready, name: mas-qa-mesh-gateway, kind: Deployment, status: InProgress
resource not ready, name: mas-qa-sync-catalog, kind: Deployment, status: InProgress
context deadline exceeded: release consul failed: resource not ready, name: consul-client, kind: DaemonSet, status: InProgress
resource not ready, name: consul-connect-injector, kind: Deployment, status: InProgress
context deadline exceeded

$ cat  maqa-spoke.yaml
global:
  name: "mas-qa"                 # UNIQUE cluster name
  datacenter: "central-primary-infra"                # Same DC is OK for OSS
  domain: consul


  # IMPORTANT: point ONLY to the HUB CONSUL SERVER VIP
  retry_join:
    - "192.168.1.99"               # consul-server LoadBalancer VIP

  tls:
    enabled: true
    enableAutoEncrypt: true
    verify: true

server:
  enabled: false                   # SPOKES NEVER RUN SERVERS

client:
  enabled: true
  grpc: true                       # REQUIRED for mesh & xDS
  exposeGossipPorts: false

connectInject:
  enabled: true
  default: false                   # opt-in per namespace

meshGateway:
  enabled: true
  mode: local                      # spoke-local gateway
  replicas: 2

ui:
  enabled: false                   # UI ONLY IN HUB

dns:
  enabled: true
  enableRedirection: true
  requestTimeout: "5s"

syncCatalog:
  enabled: true
  toConsul: true
  fromConsul: false

hostNetwork: false


#####
# switch kubeconfig context FIRST
kubectl config use-context spoke-42

# then copy the CA
kubectl -n consul get secret consul-ca-cert -o yaml \
  --context hub \
  | kubectl apply -f -


Official references (exactly this behavior)

HashiCorp docs (relevant sections):

TLS Auto Encrypt with External Agents

https://developer.hashicorp.com/consul/docs/security/encryption/tls#auto-encrypt

Consul Helm chart TLS behavior

https://developer.hashicorp.com/consul/docs/k8s/helm#tls-configuration

Multi-cluster with shared CA (OSS)

https://developer.hashicorp.com/consul/docs/connect/gateways/mesh-gateway

These docs assume:

â€œOperators distribute the CA certificate to all client environmentsâ€


ðŸ“š Supporting Official Docs

Here are the relevant official references:

ðŸ”¹ Consul Helm Chart Reference â€” global.name:
Defines global.name as the resource naming prefix:
ðŸ”— https://developer.hashicorp.com/consul/docs/reference/k8s/helm#global-name

ðŸ”¹ Consul Kubernetes Deployment Guide (general Helm usage):
Shows example usage of global.name:
ðŸ”— https://developer.hashicorp.com/consul/docs/deploy/server/k8s/helm

ðŸ”¹ Consul Multi-cluster Deployments:
Notes that Helm release names or global.name must be unique across clusters to avoid ACL collisions:
ðŸ”— https://developer.hashicorp.com/consul/docs/deploy/server/k8s/multi-cluster




 kubectl describe pods -n consul mas-qa-connect-injector-85dc48d599-6wfqw
Name:             mas-qa-connect-injector-85dc48d599-6wfqw
Namespace:        consul
Priority:         0
Service Account:  mas-qa-connect-injector
Node:             klusterx-worker-mas-qa-01/10.19.143.64
Start Time:       Mon, 26 Jan 2026 20:41:51 +0000
Labels:           app=consul
                  chart=consul-helm
                  component=connect-injector
                  pod-template-hash=85dc48d599
                  release=consul
Annotations:      consul.hashicorp.com/connect-inject: false
                  consul.hashicorp.com/mesh-inject: false
Status:           Running
IP:               10.244.1.136
IPs:
  IP:           10.244.1.136
Controlled By:  ReplicaSet/mas-qa-connect-injector-85dc48d599
Containers:
  sidecar-injector:
    Container ID:    containerd://f70f8e1c66ac7bc2171c0e57a9d441a041d40ce96dbe438069eec992dc1cda35
    Image:           hashicorp/consul-k8s-control-plane:1.9.2
    Image ID:        docker.io/hashicorp/consul-k8s-control-plane@sha256:5a306d341143577b1131788bc134c3411a3cf656310c1d3dcfc7e6a38052951e
    Port:            8080/TCP (webhook-server)
    Host Port:       0/TCP (webhook-server)
    SeccompProfile:  RuntimeDefault
    Command:
      /bin/sh
      -ec
      exec consul-k8s-control-plane inject-connect \
        -config-file=/consul/config/config.json \
        -log-level=info \
        -log-json=false \
        -default-inject=false \
        -consul-image="hashicorp/consul:1.22.2" \
        -consul-dataplane-image="hashicorp/consul-dataplane:1.9.2" \
        -consul-k8s-image="hashicorp/consul-k8s-control-plane:1.9.2" \
        -release-name="consul" \
        -release-namespace="consul" \
        -resource-prefix=mas-qa \
        -listen=:8080 \
        -default-enable-transparent-proxy=true \
        -enable-cni=false \
        -transparent-proxy-default-overwrite-probes=true \
        -enable-consul-dns=true \
        -default-enable-metrics=false \
        -enable-gateway-metrics=true  \
        -default-enable-metrics-merging=false  \
        -default-merged-metrics-port=20100 \
        -default-prometheus-scrape-port=20200 \
        -default-prometheus-scrape-path="/metrics" \
        -allow-k8s-namespace="*" \
        -tls-cert-dir=/etc/connect-injector/certs \
        -default-envoy-proxy-concurrency=2 \
        -default-enable-sidecar-proxy-lifecycle=true \
        -default-enable-sidecar-proxy-lifecycle-shutdown-drain-listeners=true \
        -default-sidecar-proxy-lifecycle-shutdown-grace-period-seconds=30 \
        -default-sidecar-proxy-lifecycle-startup-grace-period-seconds=0 \
        -default-sidecar-proxy-lifecycle-graceful-port=20600 \
        -default-sidecar-proxy-lifecycle-graceful-shutdown-path="/graceful_shutdown" \
        -default-sidecar-proxy-lifecycle-graceful-startup-path="/graceful_startup" \
        -default-sidecar-proxy-startup-failure-seconds=0 \
        -default-sidecar-proxy-liveness-failure-seconds=0 \
        -init-container-memory-limit=150Mi \
        -init-container-memory-request=25Mi \
        -init-container-cpu-request=50m \
        -enable-auto-encrypt \
        -enable-telemetry-collector=false  \
        -default-enable-consul-dataplane-as-sidecar=false \

    State:          Running
      Started:      Mon, 26 Jan 2026 20:42:52 +0000
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Mon, 26 Jan 2026 20:41:53 +0000
      Finished:     Mon, 26 Jan 2026 20:42:51 +0000
    Ready:          False
    Restart Count:  1
    Limits:
      cpu:     50m
      memory:  200Mi
    Requests:
      cpu:      50m
      memory:   200Mi
    Liveness:   http-get http://:9445/readyz/ready delay=1s timeout=5s period=10s #success=1 #failure=2
    Readiness:  http-get http://:9445/readyz/ready delay=2s timeout=5s period=10s #success=1 #failure=2
    Startup:    http-get http://:9445/readyz/ready delay=30s timeout=5s period=2s #success=1 #failure=15
    Environment:
      NAMESPACE:           consul (v1:metadata.namespace)
      POD_NAME:            mas-qa-connect-injector-85dc48d599-6wfqw (v1:metadata.name)
      CONSUL_DUAL_STACK:   false
      CONSUL_ADDRESSES:    mas-qa-server.consul.svc
      CONSUL_GRPC_PORT:    8502
      CONSUL_HTTP_PORT:    8501
      CONSUL_DATACENTER:   central-primary-infra
      CONSUL_API_TIMEOUT:  5s
      CONSUL_USE_TLS:      true
      CONSUL_CACERT_FILE:  /consul/tls/ca/tls.crt
    Mounts:
      /consul/config from config (ro)
      /consul/tls/ca from consul-ca-cert (ro)
      /etc/connect-injector/certs from certs (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qcfn2 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True
  Initialized                 True
  Ready                       False
  ContainersReady             False
  PodScheduled                True
Volumes:
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      mas-qa-connect-inject-config
    Optional:  false
  certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  mas-qa-connect-inject-webhook-cert
    Optional:    false
  consul-ca-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  consul-ca-cert
    Optional:    false
  kube-api-access-qcfn2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    Optional:                false
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                 From               Message
  ----     ------     ----                ----               -------
  Normal   Scheduled  88s                 default-scheduler  Successfully assigned consul/mas-qa-connect-injector-85dc48d599-6wfqw to klusterx-worker-mas-qa-01
  Normal   Pulled     28s (x2 over 87s)   kubelet            Container image "hashicorp/consul-k8s-control-plane:1.9.2" already present on machine
  Normal   Created    28s (x2 over 87s)   kubelet            Created container: sidecar-injector
  Warning  Unhealthy  28s (x15 over 56s)  kubelet            Startup probe failed: Get "http://10.244.1.136:9445/readyz/ready": dial tcp 10.244.1.136:9445: connect: connection refused
  Normal   Killing    28s                 kubelet            Container sidecar-injector failed startup probe, will be restarted
  Normal   Started    27s (x2 over 86s)   kubelet            Started container sidecar-injector

PAA3LIS@LIS-C-002L2 MINGW64 ~
$ kubectl describe pods -n consul mas-qa-client-2mqkm
Name:             mas-qa-client-2mqkm
Namespace:        consul
Priority:         0
Service Account:  mas-qa-client
Node:             klusterx-worker-mas-qa-02/10.19.143.65
Start Time:       Mon, 26 Jan 2026 20:41:56 +0000
Labels:           app=consul
                  chart=consul-helm
                  component=client
                  controller-revision-hash=d6894644c
                  hasDNS=true
                  pod-template-generation=1
                  release=consul
Annotations:      consul.hashicorp.com/config-checksum: 7dcb30a8ae11c9fa48a8533bf64c08674660084f73a9fce9bf22f8972cb24325
                  consul.hashicorp.com/connect-inject: false
                  consul.hashicorp.com/mesh-inject: false
Status:           Running
IP:               10.244.2.173
IPs:
  IP:           10.244.2.173
Controlled By:  DaemonSet/mas-qa-client
Containers:
  consul:
    Container ID:  containerd://aa3fe92e42e06c5b9f44c4d293b5656a8cab12941dc41d93b1dbdaf1d6da7e44
    Image:         hashicorp/consul:1.22.2
    Image ID:      docker.io/hashicorp/consul@sha256:adc4045482dec0ced2cacfaa71db1c62653a7f720b3b1698e2be09b3ec115615
    Ports:         8501/TCP (https), 8502/TCP (grpc), 8301/TCP (serflan-tcp), 8301/UDP (serflan-udp), 8600/TCP (dns-tcp), 8600/UDP (dns-udp)
    Host Ports:    8501/TCP (https), 8502/TCP (grpc), 0/TCP (serflan-tcp), 0/UDP (serflan-udp), 0/TCP (dns-tcp), 0/UDP (dns-udp)
    Command:
      /bin/sh
      -ec
      CONSUL_FULLNAME="mas-qa"

      cp /consul/tmp/extra-config/extra-from-values.json /consul/extra-config/extra-from-values.json
      [ -n "${HOST_IP}" ] && sed -Ei "s|HOST_IP|${HOST_IP?}|g" /consul/extra-config/extra-from-values.json
      [ -n "${POD_IP}" ] && sed -Ei "s|POD_IP|${POD_IP?}|g" /consul/extra-config/extra-from-values.json
      [ -n "${HOSTNAME}" ] && sed -Ei "s|HOSTNAME|${HOSTNAME?}|g" /consul/extra-config/extra-from-values.json

      exec /usr/local/bin/docker-entrypoint.sh consul agent \
        -node="${NODE}" \
        -advertise="${ADVERTISE_IP}" \
        -bind=0.0.0.0 \
        -client=0.0.0.0 \
        -node-meta=host-ip:${HOST_IP} \
        -node-meta=pod-name:${HOSTNAME} \
        -hcl='leave_on_terminate = true' \
        -hcl='ca_file = "/consul/tls/ca/tls.crt"' \
        -hcl='auto_encrypt = {tls = true}' \
        -hcl="auto_encrypt = {ip_san = [\"$HOST_IP\",\"$POD_IP\"]}" \
        -hcl='verify_outgoing = true' \
        -hcl='ports { https = 8501 }' \
        -hcl='ports { http = -1 }' \
        -hcl='ports { grpc = -1, grpc_tls = 8502 }' \
        -config-dir=/consul/config \
        -datacenter=central-primary-infra \
        -data-dir=/consul/data \
        -config-dir=/consul/extra-config \
        -domain=consul

    State:          Running
      Started:      Mon, 26 Jan 2026 20:41:57 +0000
    Ready:          False
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  100Mi
    Requests:
      cpu:      100m
      memory:   100Mi
    Readiness:  exec [/bin/sh -ec curl \
  -k \
  https://127.0.0.1:8501/v1/status/leader \
2>/dev/null | grep -E '".+"'
] delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      ADVERTISE_IP:               (v1:status.podIP)
      NAMESPACE:                 consul (v1:metadata.namespace)
      NODE:                       (v1:spec.nodeName)
      HOST_IP:                    (v1:status.hostIP)
      POD_IP:                     (v1:status.podIP)
      CONSUL_DISABLE_PERM_MGMT:  true
      CONSUL_HTTP_ADDR:          https://localhost:8501
      CONSUL_HTTP_SSL_VERIFY:    false
    Mounts:
      /consul/config from config (rw)
      /consul/data from data (rw)
      /consul/extra-config from extra-config (rw)
      /consul/login from consul-data (ro)
      /consul/tls/ca from consul-ca-cert (ro)
      /consul/tmp/extra-config from tmp-extra-config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jtwqr (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True
  Initialized                 True
  Ready                       False
  ContainersReady             False
  PodScheduled                True
Volumes:
  data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:
    SizeLimit:  <unset>
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      mas-qa-client-config
    Optional:  false
  extra-config:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:
    SizeLimit:  <unset>
  consul-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  tmp-extra-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      mas-qa-client-tmp-extra-config
    Optional:  false
  consul-ca-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  consul-ca-cert
    Optional:    false
  kube-api-access-jtwqr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    Optional:                false
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                  From               Message
  ----     ------     ----                 ----               -------
  Normal   Scheduled  2m44s                default-scheduler  Successfully assigned consul/mas-qa-client-2mqkm to klusterx-worker-mas-qa-02
  Normal   Pulled     2m45s                kubelet            Container image "hashicorp/consul:1.22.2" already present on machine
  Normal   Created    2m45s                kubelet            Created container: consul
  Normal   Started    2m44s                kubelet            Started container consul
  Warning  Unhealthy  6s (x19 over 2m43s)  kubelet            Readiness probe failed:




   Auto-Encrypt-TLS: true                                                                                                                   â”‚
â”‚            ACL Enabled: false                                                                                                                  â”‚â”‚     ACL Default Policy: allow                                                                                                                  â”‚
â”‚              HTTPS TLS: Verify Incoming: false, Verify Outgoing: true, Min Version: TLSv1_2                                                    â”‚â”‚               gRPC TLS: Verify Incoming: false, Min Version: TLSv1_2                                                                           â”‚
â”‚       Internal RPC TLS: Verify Incoming: false, Verify Outgoing: true (Verify Hostname: false), Min Version: TLSv1_2                           â”‚â”‚                                                                                                                                                â”‚
â”‚ ==> Log data will now stream in as it occurs:                                                                                                  â”‚
â”‚                                                                                                                                                â”‚
â”‚ 2026-01-26T20:42:05.953Z [WARN]  agent: The 'ca_file' field is deprecated. Use the 'tls.defaults.ca_file' field instead.                       â”‚
â”‚ 2026-01-26T20:42:05.953Z [WARN]  agent: The 'verify_outgoing' field is deprecated. Use the 'tls.defaults.verify_outgoing' field instead.       â”‚
â”‚ 2026-01-26T20:42:06.232Z [WARN]  agent.auto_config: The 'ca_file' field is deprecated. Use the 'tls.defaults.ca_file' field instead.           â”‚
â”‚ 2026-01-26T20:42:06.232Z [WARN]  agent.auto_config: The 'verify_outgoing' field is deprecated. Use the 'tls.defaults.verify_outgoing' field in â”‚
â”‚ 2026-01-26T20:42:06.232Z [ERROR] agent.auto_config: no auto-encrypt server addresses available for use                                         â”‚
â”‚ 2026-01-26T20:42:06.232Z [ERROR] agent.auto_config: no auto-encrypt server addresses available for use                                         â”‚
â”‚ 2026-01-26T20:42:07.277Z [ERROR] agent.auto_config: no auto-encrypt server addresses available for use                                         â”‚
â”‚ 2026-01-26T20:42:09.686Z [ERROR] agent.auto_config: no auto-encrypt server addresses available for use                                         â”‚
â”‚ 2026-01-26T20:42:13.780Z [ERROR] agent.auto_config: no auto-encrypt server addresses available for use                                         â”‚
â”‚ 2026-01-26T20:42:22.938Z [ERROR] agent.auto_config: no auto-encrypt server addresses available for use                                         â”‚
â”‚ 2026-01-26T20:42:42.444Z [ERROR] agent.auto_config: no auto-encrypt server addresses available for use                                         â”‚
â”‚ 2026-01-26T20:43:17.167Z [ERROR] agent.auto_config: no auto-encrypt server addresses available for use                                         â”‚
â”‚ 2026-01-26T20:44:21.974Z [ERROR] agent.auto_config: no auto-encrypt server addresses available for use                                         â”‚
â”‚ 2026-01-26T20:46:59.032Z [ERROR] agent.auto_config: no auto-encrypt server addresses available for use              2026-01-26T21:07:14.752Z [INFO]  consul-server-connection-manager.consul-server-connection-manager: trying to connect to a Consul server       â”‚
â”‚ 2026-01-26T21:07:14.755Z [ERROR] consul-server-connection-manager.consul-server-connection-manager: connection error: error="failed to discove â”‚â”‚ 2026-01-26T21:07:15.339Z [INFO]  consul-server-connection-manager.consul-server-connection-manager: trying to connect to a Consul server       â”‚
â”‚ 2026-01-26T21:07:15.341Z [ERROR] consul-server-connection-manager.consul-server-connection-manager: connection error: error="failed to discove â”‚â”‚ 2026-01-26T21:07:16.437Z [INFO]  consul-server-connection-manager.consul-server-connection-manager: trying to connect to a Consul server       â”‚
â”‚ 2026-01-26T21:07:16.440Z [ERROR] consul-server-connection-manager.consul-server-connection-manager: connection error: error="failed to discove â”‚â”‚ 2026-01-26T21:07:17.276Z [INFO]  consul-server-connection-manager.consul-server-connection-manager: trying to connect to a Consul server       â”‚
â”‚ 2026-01-26T21:07:17.279Z [ERROR] consul-server-connection-manager.consul-server-connection-manager: connection error: error="failed to discove â”‚
â”‚ 2026-01-26T21:07:19.533Z [INFO]  consul-server-connection-manager.consul-server-connection-manager: trying to connect to a Consul server       â”‚
â”‚ 2026-01-26T21:07:19.535Z [ERROR] consul-server-connection-manager.consul-server-connection-manager: connection error: error="failed to discove â”‚
â”‚ 2026-01-26T21:07:22.144Z [INFO]  consul-server-connection-manager.consul-server-connection-manager: trying to connect to a Consul server       â”‚
â”‚ 2026-01-26T21:07:22.147Z [ERROR] consul-server-connection-manager.consul-server-connection-manager: connection error: error="failed to discove â”‚
â”‚ 2026-01-26T21:07:24.164Z [INFO]  consul-server-connection-manager.consul-server-connection-manager: trying to connect to a Consul server       â”‚
â”‚ 2026-01-26T21:07:24.167Z [ERROR] consul-server-connection-manager.consul-server-connection-manager: connection error: error="failed to discove â”‚
â”‚ 2026-01-26T21:07:31.336Z [INFO]  consul-server-connection-manager.consul-server-connection-manager: trying to connect to a Consul server       â”‚
â”‚ 2026-01-26T21:07:31.339Z [ERROR] consul-server-connection-manager.consul-server-connection-manager: connection error: error="failed to discove â”‚
â”‚ 2026-01-26T21:07:42.061Z [INFO]  consul-server-connection-manager.consul-server-connection-manager: trying to connect to a Consul server       â”‚
â”‚ 2026-01-26T21:07:42.073Z [ERROR] consul-server-connection-manager.consul-server-connection-manager: connection error: error="failed to discove â”‚
â”‚ 2026-01-26T21:07:52.433Z [INFO]  consul-server-connection-manager.consul-server-connection-manager: trying to connect to a Consul server       â”‚
â”‚ 2026-01-26T21:07:52.438Z [ERROR] consul-server-connection-manager.consul-server-connection-manager: connection error: error="failed to discove â”‚
â”‚ unable to start Consul server watcher: context canceled                                                                                        â”‚
â”‚ 2026-01-26T21:08:13.398Z [INFO]  consul-server-connection-manager.consul-server-connection-manager: stopping                                   â”‚
â”‚ stream closed: EOF for consul/mas-qa-connect-injector-85dc48d599-6wfqw (sidecar-injector)                      2026-01-26T21:04:42.184Z [ERROR] consul-server-connection-manager: connection error: error="failed to discover Consul server addresses: failed â”‚
â”‚ 2026-01-26T21:05:32.034Z [INFO]  consul-server-connection-manager: trying to connect to a Consul server                                        â”‚
â”‚ 2026-01-26T21:05:32.039Z [ERROR] consul-server-connection-manager: connection error: error="failed to discover Consul server addresses: failed â”‚
â”‚ 2026-01-26T21:06:21.140Z [INFO]  consul-server-connection-manager: trying to connect to a Consul server                                        â”‚
â”‚ 2026-01-26T21:06:21.143Z [ERROR] consul-server-connection-manager: connection error: error="failed to discover Consul server addresses: failed â”‚
â”‚ 2026-01-26T21:07:14.047Z [INFO]  consul-server-connection-manager: trying to connect to a Consul server                                        â”‚
â”‚ 2026-01-26T21:07:14.057Z [ERROR] consul-server-connection-manager: connection error: error="failed to discover Consul server addresses: failed â”‚
â”‚ 2026-01-26T21:08:21.184Z [INFO]  consul-server-connection-manager: trying to connect to a Consul server                                        â”‚
â”‚ 2026-01-26T21:08:21.188Z [ERROR] consul-server-connection-manager: connection error: error="failed to discover Consul server addresses: failed â”‚
â”‚ 2026-01-26T21:09:05.393Z [INFO]  consul-server-connection-manager: trying to connect to a Consul server                                        â”‚
â”‚ 2026-01-26T21:09:05.396Z [ERROR] consul-server-connection-manager: connection error: error="failed to discover Consul server addresses: failed â”‚
â”‚ 2026-01-26T21:10:20.345Z [INFO]  consul-server-connection-manager: trying to connect to a Consul server                                        â”‚
â”‚ 2026-01-26T21:10:20.351Z [ERROR] consul-server-connection-manager: connection error: error="failed to discover Consul server addresses: failed â”‚
â”‚                  2026-01-26T21:09:51.910Z [INFO]  consul-server-connection-manager: trying to connect to a Consul server                                        â”‚
â”‚ 2026-01-26T21:09:51.913Z [ERROR] consul-server-connection-manager: connection error: error="failed to discover Consul server addresses: failed â”‚
â”‚ 2026-01-26T21:09:56.385Z [INFO]  consul-server-connection-manager: trying to connect to a Consul server                                        â”‚
â”‚ 2026-01-26T21:09:56.388Z [ERROR] consul-server-connection-manager: connection error: error="failed to discover Consul server addresses: failed â”‚
â”‚ 2026-01-26T21:10:02.910Z [INFO]  consul-server-connection-manager: trying to connect to a Consul server                                        â”‚
â”‚ 2026-01-26T21:10:02.914Z [ERROR] consul-server-connection-manager: connection error: error="failed to discover Consul server addresses: failed â”‚
â”‚ 2026-01-26T21:10:11.061Z [INFO]  consul-server-connection-manager: trying to connect to a Consul server                                        â”‚
â”‚ 2026-01-26T21:10:11.064Z [ERROR] consul-server-connection-manager: connection error: error="failed to discover Consul server addresses: failed â”‚
â”‚ unable to start Consul server watcher: context canceled                                                                                        â”‚
â”‚ 2026-01-26T21:10:26.063Z [INFO]  consul-server-connection-manager: stopping                                                                    â”‚
â”‚ stream closed: EOF for consul/mas-qa-sync-catalog-6f58865d4c-558dx (sync-catalog)                                                                                                                               2026-01-26T20:42:20.493Z [INFO]  Updated certificate bundle received for mas-qa-connect-injector; Updating webhook certs.                      â”‚
â”‚ 2026-01-26T20:42:20.551Z [INFO]  Updating secret with new certificate: mutatingwebhookconfig=mas-qa-connect-injector secret=mas-qa-connect-inj â”‚â”‚ 2026-01-26T20:42:20.556Z [INFO]  Updating webhook configuration with new CA: mutatingwebhookconfig=mas-qa-connect-injector secret=mas-qa-conne â”‚
â”‚                       




kubectl -n consul get secret mas-qa-ca-cert -o jsonpath='{.data.tls\.crt}' | base64 --decode > hub-ca.crt
kubectl -n consul create secret generic consul-ca-cert \
  --from-file=tls.crt=hub-ca.crt
        kubectl -n consul get secret consul-ca-cert
kubectl -n consul get secret consul-ca-cert -o yaml
                             
kubectl -n consul exec -it mas-qa-client-2mqkm -- curl -vk https://mas-qa-server.consul.svc:8501/v1/status/leader
kubectl -n consul exec -it mas-qa-client-2mqkm -- grpcurl -insecure mas-qa-server.consul.svc:8502


kubectl -n consul exec -it mas-qa-client-2mqkm -- curl -vk https://mas-qa-server.consul.svc:8501/v1/status/leader
* Could not resolve host: mas-qa-server.consul.svc
* shutting down connection #0
curl: (6) Could not resolve host: mas-qa-server.consul.svc
command terminated with exit code 6

PAA3LIS@LIS-C-002L2 MINGW64 ~
$ kubectl -n consul exec -it mas-qa-client-2mqkm -- grpcurl -insecure mas-qa-server.consul.svc:8502
error: Internal error occurred: Internal error occurred: error executing command in container: failed to exec in container: failed to start exec "a95900a599a59260f67841cc478860b6a233902434798f7a2c8bf7a8b36c5246": OCI runtime exec failed: exec failed: unable to start container process: exec: "grpcurl": executable file not found in $PATH


global:
  name: consul-spoke
  datacenter: mas-qa
  serverAddresses: ["<hub-ip>:8502"] # hub gRPC endpoint

connect:
  enabled: true
  autoEncrypt:
    enabled: true

tls:
  enabled: true
  verify: true
  secretName: consul-ca-cert
  secretKey: tls.crt

server:
  replicas: 0  # client-only cluster



connectInject:
  enabled: true
  default: false
  extraEnv:
    - name: CONSUL_HTTP_ADDR
      value: "http://127.0.0.1:8500"
    - name: CONSUL_GRPC_ADDR
      value: "127.0.0.1:8502"
    - name: CONSUL_USE_TLS
      value: "false"

syncCatalog:
  enabled: true
  toConsul: true
  fromConsul: false
  extraEnv:
    - name: CONSUL_HTTP_ADDR
      value: "http://127.0.0.1:8500"
    - name: CONSUL_GRPC_ADDR
      value: "127.0.0.1:8502"
    - name: CONSUL_USE_TLS
      value: "false"

meshGateway:
  enabled: true
  mode: local
  replicas: 2
  extraEnv:
    - name: CONSUL_HTTP_ADDR
      value: "http://127.0.0.1:8500"
    - name: CONSUL_GRPC_ADDR
      value: "127.0.0.1:8502"
    - name: CONSUL_USE_TLS
      value: "false"




global:
  name: spoke-mas-qa
  datacenter: mas-qa
  gossipEncryption: true # optional for WAN gossip
  tls:
    enabled: true
    caSecretName: consul-ca-cert
    caCertPath: /consul/tls/ca/tls.crt

connectInject:
  enabled: true
  default: false
  extraEnv:
    - name: CONSUL_HTTP_ADDR
      value: "https://hub.consul.global:8501"
    - name: CONSUL_GRPC_ADDR
      value: "hub.consul.global:8502"
    - name: CONSUL_USE_TLS
      value: "true"
    - name: CONSUL_CACERT_FILE
      value: "/consul/tls/ca/tls.crt"

syncCatalog:
  enabled: true
  toConsul: true
  fromConsul: false
  extraEnv:
    - name: CONSUL_HTTP_ADDR
      value: "https://hub.consul.global:8501"
    - name: CONSUL_GRPC_ADDR
      value: "hub.consul.global:8502"
    - name: CONSUL_USE_TLS
      value: "true"
    - name: CONSUL_CACERT_FILE
      value: "/consul/tls/ca/tls.crt"

meshGateway:
  enabled: true
  mode: global
  replicas: 2
  extraEnv:
    - name: CONSUL_HTTP_ADDR
      value: "https://hub.consul.global:8501"
    - name: CONSUL_GRPC_ADDR
      value: "hub.consul.global:8502"
    - name: CONSUL_USE_TLS
      value: "true"
    - name: CONSUL_CACERT_FILE
      value: "/consul/tls/ca/tls.crt"



########################################################################################################################################



# values-client-to-remote-consul.yaml
# Purpose: Install Consul on this Talos cluster in **client-only** mode and point it to
# remote Consul servers whose IPs are advertised via BIRD2 (BGP) in another cluster.
#
# Usage:
#   helm repo add hashicorp https://helm.releases.hashicorp.com
#   helm repo update
#   helm upgrade --install consul hashicorp/consul \
#     -n consul --create-namespace \
#     -f values-client-to-remote-consul.yaml
#
# Replace all REPLACE_WITH_* placeholders below before applying.

global:
  name: consul
  datacenter: REPLACE_WITH_DATACENTER_NAME   # e.g., mas-qa (must match your remote Consul DC)

  # TLS is required when connecting to the remote servers securely.
  tls:
    enabled: true
    caCert:
      secretName: consul-ca-cert   # Provided in secret-consul-ca-cert.yaml
      secretKey: tls.crt

  # Keep ACLs managed externally (aligns with your current ACL disabled setup).
  acls:
    manageSystemACLs: false

# We DO NOT run servers in this cluster.
server:
  enabled: false

# Externally hosted servers (in the other Talos cluster) exposed via BIRD2-advertised IPs.
externalServers:
  enabled: true
  # List at least 1-3 reachable server IPs or DNS names.
  hosts:
    - REPLACE_WITH_SERVER_IP_1   # e.g., 10.10.10.11
    - REPLACE_WITH_SERVER_IP_2   # e.g., 10.10.10.12
    - REPLACE_WITH_SERVER_IP_3   # e.g., 10.10.10.13
  httpsPort: 8501
  grpcPort: 8502

# Consul client agents (DaemonSet) are needed for service mesh sidecars in K8s.
client:
  enabled: true
  # Keep the client small but set sane requests/limits per CNCF & Well-Architected.
  resources:
    requests:
      cpu: 100m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi
  # Optional: anti-affinity is enabled by default in the chart, but ensure at least 1 per node.
  # If Talos nodes are tainted, add tolerations here.
  tolerations: []
  # Extra agent configuration to:
  #  - enable Auto-Encrypt to fetch client certs from servers
  #  - configure modern TLS fields (tls.defaults.*)
  #  - ensure gRPC port is enabled
  extraConfig: |
    {
      "auto_encrypt": { "tls": true },
      "ports": { "grpc": 8502 },
      "retry_join": [
        "REPLACE_WITH_SERVER_IP_1",
        "REPLACE_WITH_SERVER_IP_2",
        "REPLACE_WITH_SERVER_IP_3"
      ],
      "tls": {
        "defaults": {
          "ca_file": "/consul/tls/ca/tls.crt",
          "verify_outgoing": true,
          "min_version": "tls12"
        },
        "https": {
          "verify_incoming": false
        },
        "grpc": {
          "verify_incoming": false
        },
        "internal_rpc": {
          "verify_incoming": false,
          "verify_outgoing": true,
          "verify_hostname": false,
          "min_version": "tls12"
        }
      }
    }

# Connect injector: mutating webhook to inject Envoy sidecars.
connectInject:
  enabled: true
  default: true
  # Use the same CA as Consul if needed internally. The webhook serving cert
  # itself is provided by the separate Secret (see secret-consul-connect-inject-webhook-cert.yaml).
  # If you run the Consul control-plane that auto-manages this secret, you can omit manual creation.
  resources:
    requests:
      cpu: 50m
      memory: 200Mi
    limits:
      cpu: 200m
      memory: 400Mi

# Optional: keep the catalog synchronized (K8s <-> Consul)
syncCatalog:
  enabled: true
  default: true

# Good defaults for production hygiene
controller:
  enabled: true
  resources:
    requests:
      cpu: 50m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 256Mi

# Mesh & gateways (optional; enable if you plan multi-cluster L7 routing)
meshGateway:
  enabled: false

ingressGateways: []
terminatingGateways: []

# NetworkPolicy to restrict egress to remote Consul servers and K8s API only.
# Enable if your cluster enforces NetworkPolicy; adjust IPs/namespaces.
# This block is illustrative; if you want it rendered by Helm, set `global.enablePodSecurityPolicies` or use a separate manifest.
# networkPolicy:
#   enabled: true
#   egress:
#     - to:
#         - ipBlock:
#             cidr: REPLACE_WITH_SERVER_IP_1/32
#         - ipBlock:
#             cidr: REPLACE_WITH_SERVER_IP_2/32
#         - ipBlock:
#             cidr: REPLACE_WITH_SERVER_IP_3/32
#       ports:
#         - protocol: TCP
#           port: 8300   # server RPC
#         - protocol: TCP
#           port: 8501   # HTTPS
#         - protocol: TCP
#           port: 8502   # gRPC
#     - to:
#         - namespaceSelector: { matchLabels: { kubernetes.io/metadata.name: kube-system } }
#       ports:
#         - protocol: TCP
#           port: 443    # K8s API Server

###########################################################$$$$$$$$$$$$$$$$$$$$$$$$#########################################################
