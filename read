In 2026, building a Well-Architected Hub-and-Spoke Service Mesh from scratch requires coordinating five core layers: IPAM (NetBox), BGP (Bird2), CNI (Cilium), Service Mesh (Consul), and Cluster Management (Omni/Talos).
Below are the exact files and configurations needed to implement this architecture.
1. Network Foundation (Bird2)
This file resides on your Proxmox gateway nodes. It handles the "North-South" reachability for your Hub Gateway VIP, ensuring all Client clusters can reach the Hub via the corporate network.
File: /etc/bird/bird.conf
bird
# 2026 High-Performance BGP Configuration
router id 10.19.143.21;
protocol device {}
protocol direct {}
# Static route for the Hub's Mesh Gateway VIP (Reserved in NetBox)
protocol static {
    ipv4;
    route 10.19.143.100/32 via "vmbr0"; # VIP for the Hub Gateway
}
# BGP Peering to Corporate Router or Cilium Nodes
protocol bgp corp_router {
    local as 64512;
    neighbor 10.19.143.1 as 64512;
    ipv4 {
        import all;
        # Export the Hub VIP so clients can find it
        export filter { if proto = "static" then accept; reject; };
    };
}

2. Cluster Orchestration (Omni Config Patch)
You apply this YAML patch via the Omni API (using a Terraform POST request) during cluster creation. It disables the legacy Flannel CNI and injects Cilium and Consul Mesh Gateways.
File: omni-cluster-patch.yaml
yaml
cluster:
  network:
    cni:
      name: none # Critical: Disables Flannel to allow Cilium eBPF
  inlineManifests:
    - name: cilium-ebpf-config
      contents: |
        bgpControlPlane:
          enabled: true
        ipam:
          mode: kubernetes
    - name: consul-mesh-gateway
      contents: |
        global:
          peering:
            enabled: true
        meshGateway:
          enabled: true
          replicas: 2
          service:
            type: LoadBalancer
            annotations:
              io.cilium/lb-ipam-ips: "10.19.143.100" # Match Bird2 VIP

3. Service Mesh (Consul Cluster Peering)
This is the "Hub-and-Spoke" Handshake. The Hub cluster acts as the Acceptor (receiving connections), and Client clusters act as Dialers (initiating connections).
A. Central Hub: Peering Acceptor
File: hub-peering-acceptor.yaml
yaml
apiVersion: consul.hashicorp.com
kind: PeeringAcceptor
metadata:
  name: client-01-peering
spec:
  peerName: client-01
Use o código com cuidado.
Action: Run consul peering generate-token -name client-01 to get the JWT token for the client.
B. Client Cluster: Peering Dialer
File: client-peering-dialer.yaml
yaml
apiVersion: consul.hashicorp.com
kind: PeeringDialer
metadata:
  name: hub-dialer
spec:
  peerName: central-hub
  # Points to the Hub VIP advertised by Bird2
  address: "10.19.143.100" 
  port: 8443
  tokenSecret:
    name: hub-peering-token
    key: data

4. Service Visibility (Exporting)
To see a client service in the Central Catalog, you must explicitly export it.
File: client-exported-services.yaml
yaml
apiVersion: consul.hashicorp.com
kind: ExportedServices
metadata:
  name: client-app-exports
spec:
  services:
    - name: client-web-app
      consumers:
        - peer: central-hub

5. Infrastructure as Code (Terraform)
This connects NetBox to the Omni API, automating the "Point-to-Point" logic you designed.
File: main.tf
hcl
# 1. Get Hub IP from NetBox (Source of Truth)
data "netbox_ip_address" "hub_vip" {
  address = "10.19.143.100/32"
}
# 2. Automated POST to Omni to patch the new cluster
resource "null_resource" "patch_omni" {
  provisioner "local-exec" {
    command = <<EOT
      curl -X POST omni.internal{var.cluster_id}/patches \
      -H "Authorization: Bearer ${var.omni_token}" \
      -d @omni-cluster-patch.yaml
    EOT
  }
}
Documentation for your Team
"The 2026 Global Fabric"
• Zero-Conf IPAM: We use ProxSyncBox to discover existing IPs (like .83, .85) and mark them in NetBox. Terraform then safely requests the Hub VIP.
• BGP Transit: Bird2 handles the "Corporate Bridge," allowing isolated clusters to reach the Hub VIP without manual WireGuard tunnels.
• L7 Decoupling: Consul Peering allows clusters with the same Pod IPs to communicate using Service Names.
• Validation: The qm reboot step in our module ensures the Talos ISO picks up these specific patches on boot.
> 






####################################################################




 Consul Dataplane (Sidecarless or Envoy-based) using Consul Cluster Peering. This allows you to bridge clusters via gRPC/xDS tunnels through the Mesh Gateway. This is free and solves the "IP overlap" problem.
1. How Client Services are seen in Central Hub (gRPC/xDS)
You do not need to expose every pod to Bird2. Instead, you only expose the Mesh Gateway of the Central Hub.
• The Bridge: The Client clusters "Dial-out" via gRPC to the Central Hub's Gateway.
• Visibility: Once the gRPC connection is established, the Client Cluster's services are synchronized to the Central Hub's Catalog.
• The Result: You see client-service.virtual.consul in the Central UI, but the traffic travels inside a secure gRPC tunnel, not via standard BGP routes.

2. Required Files to Build from Scratch (Free/2026)
A. Central Hub: Peering Acceptor (hub-acceptor.yaml)
This creates the "Listener" on the Central Hub.
yaml
apiVersion: consul.hashicorp.com
kind: PeeringAcceptor
metadata:
  name: client-01-acceptor
spec:
  peerName: client-01
  # The ONLY IP Bird2 needs to know
  serverExternalAddresses: 
    - "10.19.143.100:8443" 
B. Client Cluster: Peering Dialer (client-dialer.yaml)
Instead of a manual token, you can use Terraform to fetch the token and apply it to the Client's local-exec flow.
yaml
apiVersion: consul.hashicorp.com
kind: PeeringDialer
metadata:
  name: hub-dialer
spec:
  peerName: central-hub
  # Client dials the Hub's gRPC/Envoy endpoint
  address: "10.19.143.100" 
  port: 8443
  tokenSecret:
    name: peering-token
    key: data
C. Omni Config Patch for Talos (omni-patch.yaml)
To make this work on Talos, you must disable the default Flannel CNI and inject the Consul Dataplane.
yaml
cluster:
  network:
    cni:
      name: none
  inlineManifests:
    - name: consul-dataplane
      contents: |
        # Standard Consul Helm values for Dataplane mode
        global:
          peering:
            enabled: true
        meshGateway:
          enabled: true
        connectInject:
          enabled: true
          transparentProxy:
            enabled: true

3. How Client Services become "Visible" in a URL
To satisfy the best practice for URL resolution:
1. DNS Forwarding: Configure your corporate DNS to forward .consul requests to the Central Hub's IP (10.19.143.100).
2. SNI Routing: When a user hits http://client-svc.service.consul, the Central Hub's Envoy Proxy reads the SNI header and tunnels the request via gRPC to the specific client cluster.

• Security: Traffic is encrypted via mTLS inside the gRPC tunnel.
• Scalability: You can add 500 clusters and only ever use ONE IP from your 255-range (The Hub IP).
• Cost: 100% Free (Consul Open Source + Talos + Cilium).




















######################################################################################################################################################################################################################################################################################################


#################################################################################################

To implement the Hub-and-Spoke Consul Peering model in 2026, you will use Bird2 to advertise exactly one IP (the Central Hub) and use Consul Peering to allow 100+ clusters to "dial-in" without needing their own IPs.
This satisfies the Well-Architected Framework by maximizing IP efficiency (saving your 255 pool) and ensuring security (no open ports on client clusters).
Step 1: NetBox Reservation (The Brain)
Before starting, mark your Central Hub VIP in NetBox as "Reserved" so no other service takes it.
• IP: 10.19.143.100 (The "Front Door")

Step 2: Bird2 Configuration (The Backbone)
On your Proxmox Gateway node, configure Bird2 to advertise only the Hub IP.
File: /etc/bird/bird.conf
bird
# Bird2 Central Hub Advertisement
protocol device {}
protocol direct {}
# Static route for the ONE Central Hub VIP
protocol static {
    ipv4;
    route 10.19.143.100/32 via "vmbr0"; 
}
# Export to Corporate Network
protocol bgp corp_router {
    local as 64512;
    neighbor 10.19.143.1 as 64512;
    ipv4 {
        export filter { if proto = "static" then accept; reject; };
    };
}

Step 3: Central Hub Setup (The Acceptor)
On your Central Cluster (Hub), create the peering point.
File: hub-mesh-config.yaml
yaml
apiVersion: consul.hashicorp.com
kind: Mesh
metadata:
  name: mesh
spec:
  peering:
    peerThroughMeshGateways: true # Allows L7 routing through the VIP
Use o código com cuidado.
File: hub-peering-acceptor.yaml
yaml
apiVersion: consul.hashicorp.com
kind: PeeringAcceptor
metadata:
  name: hub-acceptor
spec:
  peerName: client-cluster-01
Use o código com cuidado.
Action: Generate the peering token to be used by the clients.
bash
consul peering generate-token -name client-cluster-01 > peering-token.jwt

Step 4: Client Cluster Setup (The Dialer)
These clusters do not have a unique IP. They "dial out" to the Hub's IP (10.19.143.100).
File: client-peering-dialer.yaml
yaml
apiVersion: consul.hashicorp.com
kind: PeeringDialer
metadata:
  name: hub-dialer
spec:
  peerName: central-hub
  # Points to the ONE IP advertised by Bird2
  address: "10.19.143.100" 
  port: 8443
  tokenSecret:
    name: peering-token-secret
    key: data

Step 5: Service Visibility (The URL)
To make a client service reachable via a URL (e.g., api.service.client-01.consul), you must export it from the client to the hub.
File: client-export.yaml
yaml
apiVersion: consul.hashicorp.com
kind: ExportedServices
metadata:
  name: client-api-exports
spec:
  services:
    - name: client-api-service
      consumers:
        - peer: central-hub

Step 6: Automation via Omni/Terraform
Use Terraform to send the Config Patch to Omni. This ensures the Cilium BGP Control Plane is ready but not conflicting.
File: omni-patch.yaml
yaml
cluster:
  network:
    cni:
      name: none # Disable Flannel for Cilium eBPF
  inlineManifests:
    - name: consul-dataplane
      contents: |
        global:
          peering:
            enabled: true
        meshGateway:
          enabled: true
          replicas: 2
        connectInject:
          enabled: true
          transparentProxy:
            enabled: true
Documentation for Jan 20 Workshop
Present this as "The Zero-IP Scalability Logic":
3. IP Efficiency: We save 100+ IPs by using a Hub-and-Spoke Dial-In model.
4. No NAT/Wireguard: We use SNI Routing via the Central Hub's Envoy.
5. Global UI: Every client service appears in the Central Hub's Consul UI automatically after peering.



Connected 
######################################################################################



he Well-Architected way to connect your Hub to Bird2 is to use the Cilium BGP Control Plane. This allows your Kubernetes cluster to "talk" BGP natively to your Bird2 nodes, making the Hub VIP reachable to all other clusters without manual static routes.
1. The Logic: How Bird Accepts the Connection
Bird2 does not "receive" a connection like a web server; it peers with your Cilium nodes.
6. Cilium acts as a BGP Speaker. It says: "I have the IP 10.19.143.100 (The Hub VIP)."
7. Bird2 acts as the BGP Neighbor. It hears this and says: "OK, I will tell the rest of the network (and the other clusters) that to reach .100, they must send traffic to you."

2. Implementation: Advertising via Cilium (The Files)
Step 1: Create the IP Pool in Cilium
This defines the VIP that you reserved in NetBox. This is the one IP that the entire 100-cluster fabric will use.
File: cilium-ip-pool.yaml
yaml
apiVersion: cilium.io/v2alpha1
kind: CiliumLoadBalancerIPPool
metadata:
  name: hub-external-pool
spec:
  blocks:
    - cidr: "10.19.143.100/32" # The Hub VIP from NetBox
Step 2: Configure Cilium BGP Peering
This tells Cilium to "call" Bird2 and advertise the IP pool created in Step 1.
File: cilium-bgp-policy.yaml
yaml
apiVersion: cilium.io/v2alpha1
kind: CiliumBGPPeeringPolicy
metadata:
  name: hub-to-bird2
spec:
  nodeSelector:
    matchLabels:
      # Only your Hub Control Plane/Gateway nodes should peer
      service-role: gateway 
  virtualRouters:
    - localASN: 64512
      exportPodCIDR: false # PROTECT: Don't leak overlapping Pod IPs
      neighbors:
        - peerAddress: "10.19.143.21/32" # Your Bird2 Node IP
          peerASN: 64512
      serviceSelector:
        matchLabels:
          # Only advertise the Consul Mesh Gateway
          app: consul-mesh-gateway 
Step 3: Configure Bird2 to Accept the Peer
On your Proxmox gateway node, Bird2 must be "waiting" for the connection from the Cilium nodes.
File: /etc/bird/bird.conf
bird
protocol bgp cilium_hub {
    local as 64512;
    # Bird2 accepts connections from the Cilium node IPs
    neighbor range 10.19.143.0/24 as 64512; 
    ipv4 {
        import all; # Bird2 learns the .100 VIP from Cilium
        export none; 
    };
}

3. Documentation for the January 20th Workshop
Action	Logic	WAF Pillar
Advertisement	Cilium BGP Control Plane	Reliability: Automated failover between nodes.
IPAM	NetBox Reserved IP (.100)	Operational Excellence: Zero manual IP guessing.
Peering	Bird2 BGP Neighbor	Performance: No NAT/Wireguard overhead.
Scaling	Hub-and-Spoke Dial-In	Cost Optimization: 100 clusters, 1 IP.

The Handshake Token (Automation Step)
Since you are using Terraform, you will use a local-exec to generate the token from the Hub and inject it into the Client cluster.
bash
# Run this on the Hub Cluster to generate the secret for Client-01
consul peering generate-token -name client-01 > client-01-token.jwt
Use o código com cuidado.
B. Client Cluster: The Peering Secret
Apply the token you generated into the Client cluster so it has permission to "Talk" to the Hub.
File: client-peering-token.yaml
yaml
apiVersion: v1
kind: Secret
metadata:
  name: hub-peering-token
type: Opaque
data:
  # The JWT token from the Hub (Base64 encoded)
  token: <BASE64_TOKEN_HERE>
Use o código com cuidado.
C. Client Cluster: The Dialer Configuration
This is the most important file. It tells the Client: "Go find the Hub at the Bird2-advertised IP."
File: client-peering-dialer.yaml
yaml
apiVersion: consul.hashicorp.com
kind: PeeringDialer
metadata:
  name: dialer-to-hub
spec:
  peerName: central-hub
  # This points to the VIP Cilium is advertising to Bird2
  address: "10.19.143.100" 
  port: 8443
  tokenSecret:
    name: hub-peering-token
    key: token
Use o código com cuidado.
D. Client Cluster: Exporting Services
To make a service in the Client cluster visible in the Central Hub Catalog (for your manager to see), you must export it.
File: client-export-service.yaml
yaml
apiVersion: consul.hashicorp.com
kind: ExportedServices
metadata:
  name: client-api-exports
spec:
  services:
    - name: client-service-name
      consumers:
        - peer: central-hub


export-all.yaml (Apply to each Client Cluster)
yaml
apiVersion: consul.hashicorp.com
kind: ExportedServices
metadata:
  name: export-all-to-hub
spec:
  services:
    - name: "*"         # Wildcard: Exports ALL services
      namespace: "*"    # (Optional) Exports from all namespaces
      consumers:
        - peer: central-hub # Peer name defined during peering







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


the Well-Architected way to build a multi-cluster fabric is Consul Cluster Peering. This replaces the old WAN Federation. It is loosely coupled, meaning if a client cluster fails, the Central Hub stays healthy.
Since you are at Bosch Portugal, your manager and the Azure Architect will appreciate this Infrastructure-as-Code approach.
Prerequisites
8. Central Hub VIP: Reserved in NetBox (e.g., 10.19.143.100).
9. Cilium BGP: Advertising this .100 IP to Bird2.
10. Consul CLI: Installed on your management machine.

Phase 1: Central Hub Configuration
Run these commands on the cluster that will act as the "Catalog Hub."
1. Install Consul (Hub Mode)
File: hub-values.yaml
yaml
global:
  name: consul
  datacenter: central-hub
  peering:
    enabled: true
meshGateway:
  enabled: true
  replicas: 2
  service:
    type: LoadBalancer
    annotations:
      io.cilium/lb-ipam-ips: "10.19.143.100" # The NetBox Reserved VIP
Command:
bash
consul-k8s install -f hub-values.yaml
2. Create the Peering Acceptor
This tells the Hub to wait for a connection from a client.
File: hub-acceptor.yaml
yaml
apiVersion: consul.hashicorp.com
kind: PeeringAcceptor
metadata:
  name: client-01
spec:
  peerName: client-01
Command:
bash
kubectl apply -f hub-acceptor.yaml
# Generate the token for the client
consul-k8s peering generate-token --peer client-01 --datacenter central-hub > peering-token.jwt

Phase 2: Client Cluster Configuration
Run these commands on each of the 100+ "Data Plane" clusters.
1. Install Consul (Client Mode)
File: client-values.yaml
yaml
global:
  name: consul
  datacenter: client-01
  peering:
    enabled: true
# No unique external IP needed for Client gateways in Dial-Out mode
meshGateway:
  enabled: true
  replicas: 1
connectInject:
  enabled: true
  transparentProxy:
    enabled: true
Command:
bash
consul-k8s install -f client-values.yaml
2. Apply the Peering Secret
Inject the token you generated from the Hub.
Command:
bash
kubectl create secret generic peering-token --from-file=data=peering-token.jwt
3. Establish the Peering Dialer
File: client-dialer.yaml
yaml
apiVersion: consul.hashicorp.com
kind: PeeringDialer
metadata:
  name: hub-dialer
spec:
  peerName: central-hub
  address: "10.19.143.100" # The Hub's VIP from Bird2
  port: 8443
  tokenSecret:
    name: peering-token
    key: data
Command:
bash
kubectl apply -f client-dialer.yaml

Phase 3: Exporting All Services (Visibility)
Run this on the Client Cluster to make it visible in the Central Hub.
File: client-export.yaml
yaml
apiVersion: consul.hashicorp.com
kind: ExportedServices
metadata:
  name: export-all
spec:
  services:
    - name: "*" # Wildcard exports ALL services to the Hub
      consumers:
        - peer: central-hub
Command:
bash
kubectl apply -f client-export.yaml

4. Justification for the Workshop (Jan 20)
Step	Technical Achievement	Salary Justification Point
BGP Hub	Built a dynamic routing entry point using Cilium + Bird2.	"I've automated the entry point for 100+ clusters."
Peering	Used L7 SNI Tunneling to bypass overlapping IPs.	"I solved the technical debt of overlapping CIDRs for free."
Dial-Out	100 clusters connected using only 1 IP from our pool.	"I saved 99 IPs, extending the life of our 255-IP range."
Wildcard	Automated global service documentation.	"The system self-documents every service created."
Final Step for You
When you meet the Azure Architect on the 20th, show the Consul UI on the Central Hub. Switch to the "Peers" tab and show client-01 as Active. Then show that all services from that client are visible in the catalog.

---------------

 cat  gateway-api-rules.yaml
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: hub-ui-gateway
  namespace: kube-system
  labels:
      advertise: "true"

  annotations:
    service.beta.kubernetes.io/cilium-loadbalancer-ipam-ips: "10.19.143.74"

spec:
  gatewayClassName: cilium
  listeners:
  - name: tls
    port: 443
    protocol: TLS
    hostname: "*.de.bosch.com"
    tls:
      mode: Passthrough

    allowedRoutes:
      namespaces:
        from: All

---
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: clusters-ui-gateway
  namespace: kube-system
  labels:
      advertise: "true"

  annotations:
    service.beta.kubernetes.io/cilium-loadbalancer-ipam-ips: "10.19.143.82"
spec:
  gatewayClassName: cilium
  listeners:
  - name: tls
    port: 443
    protocol: TLS
    hostname: "*.clusters.de.bosch.com"
    tls:
      mode: Passthrough

    allowedRoutes:
      namespaces:
        from: All


PAA3LIS@LIS-C-002L2 MINGW64 ~
$ cat gateway-UI-routes.yaml
apiVersion: gateway.networking.k8s.io/v1alpha2

kind: TLSRoute
metadata:
  name: consul-ui
  namespace: consul
spec:
  parentRefs:
  - name: hub-ui-gateway
    namespace: kube-system
  hostnames:
  - consul-klusterx.de.bosch.com
  rules:
  - backendRefs:
    - name: consul-ui
      port: 443
---


apiVersion: gateway.networking.k8s.io/v1alpha2

kind: TLSRoute
metadata:
  name: rancher
  namespace: cattle-system
spec:
  parentRefs:
  - name: hub-ui-gateway
    namespace: kube-system
  hostnames:
  - rancher-klusterx.de.bosch.com
  rules:
  - backendRefs:
    - name: rancher
      port: 443

---


apiVersion: gateway.networking.k8s.io/v1alpha2
kind: TLSRoute
metadata:
  name: remote-clusters
  namespace: consul
spec:
  parentRefs:
  - name: clusters-ui-gateway
    namespace: kube-system
  hostnames:
  - "*.clusters.de.bosch.com"
  rules:
  - backendRefs:
    - name: consul-mesh-gateway
      port: 8443


