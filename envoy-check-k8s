im going roud now i need fix root@klusterx-envoyingress-01:~# consul config read -kind tcp-route -name hub-tcp
consul config read -kind service-defaults -name hub-egress
consul config read -kind terminating-gateway -name tg-egress
curl -s --noproxy localhost http://localhost:9903/clusters | sed -n '/^hub-egress\./,/^$/p'
{
    "Kind": "tcp-route",
    "Name": "hub-tcp",
    "Parents": [
        {
            "Kind": "api-gateway",
            "Name": "edge-gw-tcp",
            "SectionName": "tcp4443"
        }
    ],
    "Services": [
        {
            "Name": "hub-egress"
        }
    ],
    "Status": {
        "Conditions": [
            {
                "Type": "Accepted",
                "Status": "True",
                "Reason": "Accepted",
                "Message": "route is valid",
                "Resource": {
                    "Kind": "",
                    "Name": "",
                    "SectionName": ""
                },
                "LastTransitionTime": "2026-02-13T00:44:10.91513962Z"
            },
            {
                "Type": "Bound",
                "Status": "True",
                "Reason": "Bound",
                "Message": "successfully bound route",
                "Resource": {
                    "Kind": "api-gateway",
                    "Name": "edge-gw-tcp",
                    "SectionName": "tcp4443"
                },
                "LastTransitionTime": "2026-02-13T00:44:10.915142833Z"
            }
        ]
    },
    "CreateIndex": 3248986,
    "ModifyIndex": 3259293
}
{
    "Kind": "service-defaults",
    "Name": "hub-egress",
    "Protocol": "tcp",
    "TransparentProxy": {},
    "MeshGateway": {},
    "Expose": {},
    "ExternalSNI": "rancher-klusterx.de.bosch.com",
    "Destination": {
        "Addresses": [
            "10.19.143.82"
        ],
        "Port": 443
    },
    "CreateIndex": 3251897,
    "ModifyIndex": 3255910
}
{
    "Kind": "terminating-gateway",
    "Name": "tg-egress",
    "Services": [
        {
            "Name": "hub-egress"
        }
    ],
    "CreateIndex": 3251989,
    "ModifyIndex": 3251989
}
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::observability_name::hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::outlier::success_rate_average::-1
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::outlier::success_rate_ejection_threshold::-1
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::outlier::local_origin_success_rate_average::-1
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::outlier::local_origin_success_rate_ejection_threshold::-1
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::default_priority::max_connections::1024
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::default_priority::max_pending_requests::1024
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::default_priority::max_requests::1024
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::default_priority::max_retries::3
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::high_priority::max_connections::1024
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::high_priority::max_pending_requests::1024
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::high_priority::max_requests::1024
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::high_priority::max_retries::3
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::added_via_api::true
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::10.19.143.93:8443::cx_active::0
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::10.19.143.93:8443::cx_connect_fail::190
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::10.19.143.93:8443::cx_total::190
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::10.19.143.93:8443::rq_active::0
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::10.19.143.93:8443::rq_error::0
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::10.19.143.93:8443::rq_success::0
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::10.19.143.93:8443::rq_timeout::0
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::10.19.143.93:8443::rq_total::0
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::10.19.143.93:8443::hostname::
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::10.19.143.93:8443::health_flags::healthy
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::10.19.143.93:8443::weight::1
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::10.19.143.93:8443::region::
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::10.19.143.93:8443::zone::
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::10.19.143.93:8443::sub_zone::
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::10.19.143.93:8443::canary::false
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::10.19.143.93:8443::priority::0
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::10.19.143.93:8443::success_rate::-1
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::10.19.143.93:8443::local_origin_success_rate::-1

.--

hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::10.19.143.93:8443::zone::
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::10.19.143.93:8443::sub_zone::
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::10.19.143.93:8443::canary::false
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::10.19.143.93:8443::priority::0
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::10.19.143.93:8443::success_rate::-1
hub-egress.default.central-primary-infra.internal.3301f5b2-7a40-f546-330a-d2ff660c775b.consul::10.19.143.93:8443::local_origin_success_rate::-1
local_agent::observability_name::local_agent
local_agent::default_priority::max_connections::1024
local_agent::default_priority::max_pending_requests::1024
local_agent::default_priority::max_requests::1024
local_agent::default_priority::max_retries::3
local_agent::high_priority::max_connections::1024
local_agent::high_priority::max_pending_requests::1024
local_agent::high_priority::max_requests::1024
local_agent::high_priority::max_retries::3
local_agent::added_via_api::false
local_agent::127.0.0.1:8502::cx_active::1
local_agent::127.0.0.1:8502::cx_connect_fail::0
local_agent::127.0.0.1:8502::cx_total::1
local_agent::127.0.0.1:8502::rq_active::1
local_agent::127.0.0.1:8502::rq_error::0
local_agent::127.0.0.1:8502::rq_success::0
local_agent::127.0.0.1:8502::rq_timeout::0
local_agent::127.0.0.1:8502::rq_total::1
local_agent::127.0.0.1:8502::hostname::
local_agent::127.0.0.1:8502::health_flags::healthy
local_agent::127.0.0.1:8502::weight::1
local_agent::127.0.0.1:8502::region::
local_agent::127.0.0.1:8502::zone::
local_agent::127.0.0.1:8502::sub_zone::
local_agent::127.0.0.1:8502::canary::false
local_agent::127.0.0.1:8502::priority::0
local_agent::127.0.0.1:8502::success_rate::-1
local_agent::127.0.0.1:8502::local_origin_success_rate::-1
root@klusterx-envoyingress-01:/etc/consul.d# 
HTTP_PROXY= HTTPS_PROXY= ALL_PROXY= http_proxy= https_proxy= all_proxy= \
curl -vk -x "" \
  https://rancher-klusterx.de.bosch.com:8443 \
  --resolve rancher-klusterx.de.bosch.com:8443:10.19.143.93
* Added rancher-klusterx.de.bosch.com:8443:10.19.143.93 to DNS cache
* Uses proxy env variable no_proxy == 'http://rb-proxy-sl.bosch.com:8008'
* Hostname rancher-klusterx.de.bosch.com was found in DNS cache
*   Trying 10.19.143.93:8443...
* Connected to rancher-klusterx.de.bosch.com (10.19.143.93) port 8443
* ALPN: curl offers h2,http/1.1
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* Recv failure: Connection reset by peer
* OpenSSL SSL_connect: Connection reset by peer in connection to rancher-klusterx.de.bosch.com:8443 
* Closing connection
curl: (35) Recv failure: Connection reset by peer
root@klusterx-envoyingress-01:/etc/consul.d# 
consul config list | grep -E 'api-gateway|tcp-route'
consul config read -kind api-gateway -name edge-gw-tcp
curl -s localhost:9903/listeners | jq '.[].name' | head
curl -s localhost:9904/listeners | jq '.[].name' | head
Must specify the -kind parameter
{
    "Kind": "api-gateway",
    "Name": "edge-gw-tcp",
    "Listeners": [
        {
            "Name": "tcp4443",
            "Hostname": "",
            "Port": 4443,
            "Protocol": "tcp",
            "TLS": {
                "Certificates": null
            }
        }
    ],
    "Status": {
        "Conditions": [
            {
                "Type": "Accepted",
                "Status": "True",
                "Reason": "Accepted",
                "Message": "gateway is valid",
                "Resource": {
                    "Kind": "",
                    "Name": "",
                    "SectionName": ""
                },
                "LastTransitionTime": "2026-02-13T00:33:19.899611938Z"
            },
            {
                "Type": "Conflicted",
                "Status": "False",
                "Reason": "NoConflict",
                "Message": "listener has no route conflicts",
                "Resource": {
                    "Kind": "api-gateway",
                    "Name": "edge-gw-tcp",
                    "SectionName": "tcp4443"
                },
                "LastTransitionTime": "2026-02-13T00:33:19.899615594Z"
            },
            {
                "Type": "ResolvedRefs",
                "Status": "True",
                "Reason": "ResolvedRefs",
                "Message": "resolved refs",
                "Resource": {
                    "Kind": "api-gateway",
                    "Name": "edge-gw-tcp",
                    "SectionName": "tcp4443"
                },
                "LastTransitionTime": "2026-02-13T00:33:19.899611367Z"
            }
        ]
    },
    "CreateIndex": 3248062,
    "ModifyIndex": 3248850
}
jq: parse error: Invalid numeric literal at line 1, column 13
jq: parse error: Invalid numeric literal at line 1, column 13
root@klusterx-envoyingress-01:/etc/consul.d# HTTP_PROXY= HTTPS_PROXY= ALL_PROXY= http_proxy= https_proxy= all_proxy= \
curl -vk -x "" \
  https://rancher-klusterx.de.bosch.com:443 \
  --resolve rancher-klusterx.de.bosch.com:8443:10.19.143.93
* Added rancher-klusterx.de.bosch.com:8443:10.19.143.93 to DNS cache
* Uses proxy env variable no_proxy == 'http://rb-proxy-sl.bosch.com:8008'
* Host rancher-klusterx.de.bosch.com:443 was resolved.
* IPv6: (none)
* IPv4: 10.19.143.74
*   Trying 10.19.143.74:443...
* Connected to rancher-klusterx.de.bosch.com (10.19.143.74) port 443
* ALPN: curl offers h2,http/1.1
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* Recv failure: Connection reset by peer
* OpenSSL SSL_connect: Connection reset by peer in connection to rancher-klusterx.de.bosch.com:443 
* Closing connection
curl: (35) Recv failure: Connection reset by peer
root@klusterx-envoyingress-01:/etc/consul.d# HTTP_PROXY= HTTPS_PROXY= ALL_PROXY= http_proxy= https_proxy= all_proxy= curl -vk -x ""   https://rancher-klusterx.de.bosch.com:443   --resolve rancher-klusterx.de.bosch.com:443:10.19.143.93
* Added rancher-klusterx.de.bosch.com:443:10.19.143.93 to DNS cache
* Uses proxy env variable no_proxy == 'http://rb-proxy-sl.bosch.com:8008'
* Hostname rancher-klusterx.de.bosch.com was found in DNS cache
*   Trying 10.19.143.93:443...
* Connected to rancher-klusterx.de.bosch.com (10.19.143.93) port 443
* ALPN: curl offers h2,http/1.1
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
* TLSv1.3 (IN), TLS handshake, Certificate (11):
* TLSv1.3 (IN), TLS handshake, CERT verify (15):
* TLSv1.3 (IN), TLS handshake, Finished (20):
* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.3 (OUT), TLS handshake, Finished (20):
* SSL connection using TLSv1.3 / TLS_CHACHA20_POLY1305_SHA256 / X25519 / RSASSA-PSS
* ALPN: server accepted http/1.1
* Server certificate:
*  subject: CN=rancher-klusterx.de.bosch.com
*  start date: Feb 12 14:17:57 2026 GMT
*  expire date: Mar 14 14:17:57 2026 GMT
*  issuer: CN=rancher-klusterx.de.bosch.com
*  SSL certificate verify result: self-signed certificate (18), continuing anyway.
*   Certificate level 0: Public key type RSA (2048/112 Bits/secBits), signed using sha256WithRSAEncryption
* using HTTP/1.x
> GET / HTTP/1.1
> Host: rancher-klusterx.de.bosch.com
> User-Agent: curl/8.5.0
> Accept: */*
> 
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* old SSL session ID is stale, removing
< HTTP/1.1 503 Service Unavailable
< content-length: 19
< content-type: text/plain
< date: Fri, 13 Feb 2026 02:22:10 GMT
< server: envoy
< 
* Connection #0 to host rancher-klusterx.de.bosch.com left intact
no healthy upstreamroot@klusterx-envoyingress-01:/etc/consul.d# HTTP_PROXY= HTTPS_PROXY= ALL_PROXY= http_proxy= https_proxy= all_proxy= curl -vk -x ""   https://ran
root@klusterx-envoyingress-01:/etc/consul.d# 
root@klusterx-envoyingress-01:/etc/consul.d# 
root@klusterx-envoyingress-01:/etc/consul.d# 
root@klusterx-envoyingress-01:/etc/consul.d# HTTP_PROXY= HTTPS_PROXY= ALL_PROXY= http_proxy= https_proxy= all_proxy= curl -vk -x ""   https://rancher-klusterx.de.bosch.com:4443   --resolve rancher-klusterx.de.bosch.com:4443:10.19.143.93
* Added rancher-klusterx.de.bosch.com:4443:10.19.143.93 to DNS cache
* Uses proxy env variable no_proxy == 'http://rb-proxy-sl.bosch.com:8008'
* Hostname rancher-klusterx.de.bosch.com was found in DNS cache
*   Trying 10.19.143.93:4443...
* Connected to rancher-klusterx.de.bosch.com (10.19.143.93) port 4443
* ALPN: curl offers h2,http/1.1
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* OpenSSL SSL_connect: SSL_ERROR_SYSCALL in connection to rancher-klusterx.de.bosch.com:4443 
* Closing connection
curl: (35) OpenSSL SSL_connect: SSL_ERROR_SYSCALL in connection to rancher-klusterx.de.bosch.com:4443 
, we arwe going arounfd to much nohting workig
-----

Good â€” this clarification is the missing puzzle piece. Now the behavior youâ€™re seeing makes sense, and yes: this is an intentional outcome of how Consul works, not a random bug.

Iâ€™ll be very precise and tie it directly to your setup (VM â†’ Envoy â†’ K8s LB via BGP).

1ï¸âƒ£ What your architecture ACTUALLY is (as Consul sees it)

Letâ€™s restate it cleanly:

Client VM (outside k8s)
  â””â”€ Envoy (running on the VM, registered in Consul)
      â””â”€ Consul service "hub-egress"
          â””â”€ Destination = K8s LoadBalancer (BGP-advertised)
              â””â”€ Rancher inside cluster


Key facts you confirmed:

10.19.143.93:8443 = the VM itself

The K8s LB IP is advertised via BGP (Bird)

The LB does NOT do port translation

You expect Consul to â€œroute through the LBâ€ automatically

This expectation is the problem.

2ï¸âƒ£ The core rule youâ€™re hitting (this is NOT documented clearly)

Consul never routes traffic â€œthroughâ€ a LoadBalancer

It routes traffic to service instances.

And service instances are defined by where Envoy is running.

What you registered

You registered the service on the VM.

So Consul does exactly this:

hub-egress â†’ 10.19.143.93:<port>


That is correct and intentional.

Consul has no concept of:

â€œthis service should forward to another IPâ€

â€œthis IP is just a hopâ€

â€œthis LB will translate portsâ€

Unless you explicitly tell it.

3ï¸âƒ£ Why the K8s LoadBalancer is invisible to Consul

Even though:

Bird advertises the LB via BGP

The VM has a route to the LB IP

Curl from the VM works

Consul still does not care, because:

âŒ The LB IP is not registered as a service instance
âŒ The LB is not part of the mesh
âŒ The LB does not run Envoy

So Consul cannot â€œtargetâ€ it implicitly.

4ï¸âƒ£ Why you see the VM IP as upstream (this is correct)

This line proves everything:

hub-egress...::10.19.143.93:8443


Envoy is connecting to the service instance address, which is:

the node where the service is registered

Since the service runs on the VM â†’ the VM IP is correct.

This is not a bug
This is not a port issue
This is not an SNI issue

This is service identity vs network routing

5ï¸âƒ£ The LB cannot â€œtranslateâ€ inside Consulâ€™s model

Even if the LB:

listens on 8443

forwards to 443

routes correctly at L4

Consul will never chain:

Envoy â†’ VM â†’ LB â†’ backend


Consul only does:

Envoy â†’ service instance


Everything after that must be handled by that instance itself.

6ï¸âƒ£ You have THREE valid ways to make this work (pick ONE)
âœ… Option 1 â€” Register the LB as the service (MOST COMMON)

Do not register the VM as hub-egress.

Instead:

service {
  name = "hub-egress"
  address = "<LB_IP>"
  port = 443
}


Then:

Envoy â†’ LB â†’ Rancher


âœ” Works with BGP
âœ” Works with no port translation
âœ” No terminating gateway needed
âœ” ExternalSNI stays top-level (as you already fixed)

âœ… Option 2 â€” VM acts as a real proxy (explicit forwarding)

If you want the VM to stay the service instance, then the VM must:

Listen on 8443

Forward traffic to the LB itself (iptables / HAProxy / Envoy static cluster)

Consul will not do that forwarding for you.

âœ… Option 3 â€” Put Envoy on the LB node (mesh-native)

This is the â€œpureâ€ mesh model:

Envoy runs where the LB lives (or replaces it)

That IP becomes the service instance

Consul routing becomes 1:1

This is how Consul expects things to work.

7ï¸âƒ£ What is NOT going to work (important)

âŒ Expecting Consul to:

discover a BGP LB

route through it

translate ports

or infer intent

Consul never does this.

8ï¸âƒ£ Direct answer to your question

â€œcheck on documentation if this is intention error or portsâ€

âœ” This is intentional behavior
âœ” Not a port bug
âœ” Not a schema issue
âœ” Not OSS vs Enterprise

It is a service-registration model mismatch.

9ï¸âƒ£ One-sentence fix (if you want it to just work)

Register the LoadBalancer IP as the service address, not the VM IP.

Everything else you already did is fine.

If you want, next I can:

Show the exact minimal HCL for Option 1

Explain how to keep security while bypassing the VM

Help you decide if you even need Consul here at all (honest answer)

Youâ€™re not â€œgoing aroundâ€ â€” you just crossed the boundary between network routing and service identity, and Consul only understands one of them.
-----




Absolutelyâ€”your writeâ€‘up is spot on. What youâ€™re seeing is intentional behavior in Consulâ€™s model: the API Gateway on VMs forwards only to services in the mesh (i.e., endpoints delivered via EDS), and authorization is enforced by service intentions. A plain catalog registration (like the LB IP by itself) wonâ€™t become an upstream for the VM API Gateway; you solved that gap by introducing a terminatingâ€‘gateway that exposes a mesh service (hubâ€‘egress) with Destination=10.19.143.82:443. That part is correct and matches the docs. [github.com]
Right now, youâ€™re blocked only by ACL: your token canâ€™t write the intention that authorizes edgeâ€‘gwâ€‘tcp â†’ hubâ€‘egress. Thatâ€™s why the edge Envoy shows:
hubâ€‘egress â€¦ ::10.19.143.93:8443::health_flags::healthy
â€¦ ::cx_connect_fail::N

The endpoint exists (TG is up), but every mTLS connection attempt is refused by intentions â†’ Envoy increments cx_connect_fail and your client socket on :4443 is accepted then closed (curl reports SSL_ERROR_SYSCALL). This is exactly how Consul behaves when the upstream cluster has no usable (authorized) endpoints. [developer....hicorp.com]
Below are the precise, minimal steps to finishâ€”no redesign, no reâ€‘wiringâ€”just fix the ACL and youâ€™re done.

âœ… What to do (in order)
1) Use a token that can write intentions
You need intentions:write to create/modify serviceâ€‘intentions. (Docs: the Config API requires specific ACLs by kind; for serviceâ€‘intentions itâ€™s intentions:write.) [discuss.ha...hicorp.com]
Option A â€“ quick test with a management token
Temporarily set a management token to perform the write, then revert:
Shellexport CONSUL_HTTP_TOKEN='<management-token>'Show more lines
Option B â€“ least privilege token
Create a policy (with a management token), then mint a token from it:
Terraform# policy: edge-gw-admin (example)node_prefix ""    { policy = "read" }service_prefix "" { policy = "read" }mesh              = "read"intentions        = "write"Show more lines
Attach this policy to a new token and:
Shellexport CONSUL_HTTP_TOKEN='<token-with-intentions:write>'Show more lines

The error you sawâ€”403 Permission denied â€¦ lacks permission 'intention:write'â€”is exactly this. Once you use a token with the right capability, the write will succeed. (The trailing â€œreferenced by gateway/routerâ€ text is an additional guard for deletes/overwrites; it doesnâ€™t prevent creating a new intention entry when you have permissions.) [discuss.ha...hicorp.com]


2) Create the allow intention (edgeâ€‘gwâ€‘tcp â†’ hubâ€‘egress)
You can write it either as a Config Entry or via the classic CLI:
Shell# Config Entry formconsul config write - <<'EOF'Kind = "service-intentions"Name = "hub-egress"Sources = [  { Name = "edge-gw-tcp", Action = "allow" }]EOFShow more lines
or
Shell# Classic â€˜intentionâ€™ commandconsul intention create -allow edge-gw-tcp hub-egressShow more lines
Verify:
Shellconsul intention list | grep -E 'hub-egress|edge-gw-tcp' || trueShow more lines

VM API Gateway routes attach to gateway listeners and forward to services in the mesh; those flows are governed by service intentions, just like any other Consul Connect traffic.


3) Reâ€‘test and watch the upstream stats flip from â€œconnect_failâ€ to â€œconnectedâ€
Client â†’ edge TCP listener (preserve SNI; bypass corp proxy):
ShellHTTP_PROXY= HTTPS_PROXY= ALL_PROXY= http_proxy= https_proxy= all_proxy= \curl -vk -x "" \  https://rancher-klusterx.de.bosch.com:4443 \  --resolve rancher-klusterx.de.bosch.com:4443:10.19.143.93``Show more lines
Edge Envoy (admin :9903) â†’ check the upstream cluster section:
Shellcurl -s --noproxy localhost http://localhost:9903/clusters \  | sed -n '/^hub-egress\./,/^$/p'Show more lines
Before the intention, this showed your host 10.19.143.93:8443 with ::cx_connect_fail::N increasing.
After the intention, the counter stops increasing; the curl returns the K8s Gateway Passthrough response (TLSRoute by SNI â†’ Rancher). Thatâ€™s the expected end state. [github.com]

ğŸ” Why your â€œrecapâ€ is correctâ€”and how it maps to Consulâ€™s model

VM API Gateway on VMs = listeners via api-gateway + routes via tcp-route/http-route. Routes forward to mesh services (i.e., endpoints delivered via EDS).
Terminatingâ€‘gateway turns an external address into a mesh service using ServiceDefaults â†’ Destination; the TG exposes that service locally (here hubâ€‘egress on :8443) and dials the external target (10.19.143.82:443). Thatâ€™s exactly what your hubâ€‘egress entry is doing. [github.com]
K8s Gateway is TLS Passthrough and routes purely by SNI to TLSRoutes (e.g., rancherâ€‘klusterx.de.bosch.com). Your TCP listener at the VM preserves SNI endâ€‘toâ€‘end, which is the correct pattern. [github.com]
Upstream must be â€œusableâ€: if the edge Envoy cannot complete an authorized mTLS to its upstream (TG), it records connect failures and your client sees SSL_ERROR_SYSCALL (or HTTP 503 in L7). This is the documented behavior (â€œno healthy upstreamâ€ / empty or unusable cluster). [developer....hicorp.com]


ğŸ§ª Oneâ€‘command â€œdoctorâ€ (optional)
If you want, Iâ€™ll paste a tiny gw_b_tg_doctor.sh that:

asserts edgeâ€‘gwâ€‘tcp and tgâ€‘egress processes,
confirms listeners :4443 (edge) and :8443 (TG),
prints intentions for edge-gw-tcp â†’ hubâ€‘egress,
dumps the :9903 hubâ€‘egress cluster section (so you see cx_connect_fail stop),
runs the canonical curl -x "" --resolve â€¦:4443:â€¦.

Say â€œdoctor script pleaseâ€ and Iâ€™ll drop it readyâ€‘toâ€‘run.

Citations

VM API Gateway routes on VMs: listeners & routes are config entries; routes forward to mesh services.
ServiceDefaults reference (including Destination, used by terminatingâ€‘gateway for external egress). [github.com]
K8s Gateway (TLS Passthrough) routes by SNI; connecting by IP without matching SNI is reset. [github.com]
Envoy/Consul behavior: no healthy/authorized upstream â†’ 503 for HTTP or TCP close; cx_connect_fail indicates upstream connection failures. [developer....hicorp.com]

Youâ€™re at the finish lineâ€”write the single intention with a token that has intentions:write, and your :4443 path will work endâ€‘toâ€‘end.
